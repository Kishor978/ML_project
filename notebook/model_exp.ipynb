{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87aa2bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f2ba6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('E:\\\\NAAMII\\\\Machine_learning\\\\dataset\\\\train_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeb63960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Feature_5</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_7</th>\n",
       "      <th>Feature_8</th>\n",
       "      <th>Feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature_3230</th>\n",
       "      <th>Feature_3231</th>\n",
       "      <th>Feature_3232</th>\n",
       "      <th>Feature_3233</th>\n",
       "      <th>Feature_3234</th>\n",
       "      <th>Feature_3235</th>\n",
       "      <th>Feature_3236</th>\n",
       "      <th>Feature_3237</th>\n",
       "      <th>Feature_3238</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_1</td>\n",
       "      <td>18281.541667</td>\n",
       "      <td>18432.0</td>\n",
       "      <td>9409.650391</td>\n",
       "      <td>0.514708</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.045369</td>\n",
       "      <td>2.803803</td>\n",
       "      <td>0.356658</td>\n",
       "      <td>1.803803</td>\n",
       "      <td>...</td>\n",
       "      <td>382.968383</td>\n",
       "      <td>2214.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>136.625113</td>\n",
       "      <td>0.061710</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.154838</td>\n",
       "      <td>4.174959</td>\n",
       "      <td>0.061710</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_2</td>\n",
       "      <td>20010.083333</td>\n",
       "      <td>20100.0</td>\n",
       "      <td>8303.049072</td>\n",
       "      <td>0.417707</td>\n",
       "      <td>0.014959</td>\n",
       "      <td>0.080294</td>\n",
       "      <td>2.338398</td>\n",
       "      <td>0.429532</td>\n",
       "      <td>1.338398</td>\n",
       "      <td>...</td>\n",
       "      <td>452.986164</td>\n",
       "      <td>2548.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>232.564022</td>\n",
       "      <td>0.090548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.934229</td>\n",
       "      <td>3.931950</td>\n",
       "      <td>0.090548</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_3</td>\n",
       "      <td>27260.125000</td>\n",
       "      <td>27437.0</td>\n",
       "      <td>12189.649414</td>\n",
       "      <td>0.447160</td>\n",
       "      <td>0.011428</td>\n",
       "      <td>0.046402</td>\n",
       "      <td>2.782842</td>\n",
       "      <td>0.359345</td>\n",
       "      <td>1.782842</td>\n",
       "      <td>...</td>\n",
       "      <td>419.781765</td>\n",
       "      <td>3400.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>233.593529</td>\n",
       "      <td>0.068704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.904807</td>\n",
       "      <td>4.085035</td>\n",
       "      <td>0.068704</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_4</td>\n",
       "      <td>41938.125000</td>\n",
       "      <td>42138.0</td>\n",
       "      <td>17866.433594</td>\n",
       "      <td>0.426019</td>\n",
       "      <td>0.009908</td>\n",
       "      <td>0.034878</td>\n",
       "      <td>3.060655</td>\n",
       "      <td>0.326727</td>\n",
       "      <td>2.060655</td>\n",
       "      <td>...</td>\n",
       "      <td>439.023968</td>\n",
       "      <td>5424.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>427.429572</td>\n",
       "      <td>0.078803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.870588</td>\n",
       "      <td>4.011726</td>\n",
       "      <td>0.078803</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_5</td>\n",
       "      <td>41274.125000</td>\n",
       "      <td>41439.0</td>\n",
       "      <td>14315.041992</td>\n",
       "      <td>0.346828</td>\n",
       "      <td>0.013596</td>\n",
       "      <td>0.065680</td>\n",
       "      <td>2.478506</td>\n",
       "      <td>0.403469</td>\n",
       "      <td>1.478506</td>\n",
       "      <td>...</td>\n",
       "      <td>485.209184</td>\n",
       "      <td>5096.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>726.731554</td>\n",
       "      <td>0.142608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.846909</td>\n",
       "      <td>3.571352</td>\n",
       "      <td>0.142608</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3240 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID     Feature_1  Feature_2     Feature_3  Feature_4  Feature_5  \\\n",
       "0  ID_1  18281.541667    18432.0   9409.650391   0.514708   0.011300   \n",
       "1  ID_2  20010.083333    20100.0   8303.049072   0.417707   0.014959   \n",
       "2  ID_3  27260.125000    27437.0  12189.649414   0.447160   0.011428   \n",
       "3  ID_4  41938.125000    42138.0  17866.433594   0.426019   0.009908   \n",
       "4  ID_5  41274.125000    41439.0  14315.041992   0.346828   0.013596   \n",
       "\n",
       "   Feature_6  Feature_7  Feature_8  Feature_9  ...  Feature_3230  \\\n",
       "0   0.045369   2.803803   0.356658   1.803803  ...    382.968383   \n",
       "1   0.080294   2.338398   0.429532   1.338398  ...    452.986164   \n",
       "2   0.046402   2.782842   0.359345   1.782842  ...    419.781765   \n",
       "3   0.034878   3.060655   0.326727   2.060655  ...    439.023968   \n",
       "4   0.065680   2.478506   0.403469   1.478506  ...    485.209184   \n",
       "\n",
       "   Feature_3231  Feature_3232  Feature_3233  Feature_3234  Feature_3235  \\\n",
       "0        2214.0           1.0    136.625113      0.061710           0.0   \n",
       "1        2548.5           1.0    232.564022      0.090548           0.0   \n",
       "2        3400.0           1.0    233.593529      0.068704           0.0   \n",
       "3        5424.0           1.0    427.429572      0.078803           0.0   \n",
       "4        5096.0           1.0    726.731554      0.142608           0.0   \n",
       "\n",
       "   Feature_3236  Feature_3237  Feature_3238  CLASS  \n",
       "0     28.154838      4.174959      0.061710      0  \n",
       "1     27.934229      3.931950      0.090548      1  \n",
       "2     27.904807      4.085035      0.068704      1  \n",
       "3     27.870588      4.011726      0.078803      0  \n",
       "4     28.846909      3.571352      0.142608      0  \n",
       "\n",
       "[5 rows x 3240 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e6c5865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Feature_5</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_7</th>\n",
       "      <th>Feature_8</th>\n",
       "      <th>Feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature_3230</th>\n",
       "      <th>Feature_3231</th>\n",
       "      <th>Feature_3232</th>\n",
       "      <th>Feature_3233</th>\n",
       "      <th>Feature_3234</th>\n",
       "      <th>Feature_3235</th>\n",
       "      <th>Feature_3236</th>\n",
       "      <th>Feature_3237</th>\n",
       "      <th>Feature_3238</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_1</td>\n",
       "      <td>18281.541667</td>\n",
       "      <td>18432.0</td>\n",
       "      <td>9409.650391</td>\n",
       "      <td>0.514708</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.045369</td>\n",
       "      <td>2.803803</td>\n",
       "      <td>0.356658</td>\n",
       "      <td>1.803803</td>\n",
       "      <td>...</td>\n",
       "      <td>382.968383</td>\n",
       "      <td>2214.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>136.625113</td>\n",
       "      <td>0.061710</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.154838</td>\n",
       "      <td>4.174959</td>\n",
       "      <td>0.061710</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_2</td>\n",
       "      <td>20010.083333</td>\n",
       "      <td>20100.0</td>\n",
       "      <td>8303.049072</td>\n",
       "      <td>0.417707</td>\n",
       "      <td>0.014959</td>\n",
       "      <td>0.080294</td>\n",
       "      <td>2.338398</td>\n",
       "      <td>0.429532</td>\n",
       "      <td>1.338398</td>\n",
       "      <td>...</td>\n",
       "      <td>452.986164</td>\n",
       "      <td>2548.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>232.564022</td>\n",
       "      <td>0.090548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.934229</td>\n",
       "      <td>3.931950</td>\n",
       "      <td>0.090548</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_3</td>\n",
       "      <td>27260.125000</td>\n",
       "      <td>27437.0</td>\n",
       "      <td>12189.649414</td>\n",
       "      <td>0.447160</td>\n",
       "      <td>0.011428</td>\n",
       "      <td>0.046402</td>\n",
       "      <td>2.782842</td>\n",
       "      <td>0.359345</td>\n",
       "      <td>1.782842</td>\n",
       "      <td>...</td>\n",
       "      <td>419.781765</td>\n",
       "      <td>3400.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>233.593529</td>\n",
       "      <td>0.068704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.904807</td>\n",
       "      <td>4.085035</td>\n",
       "      <td>0.068704</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_4</td>\n",
       "      <td>41938.125000</td>\n",
       "      <td>42138.0</td>\n",
       "      <td>17866.433594</td>\n",
       "      <td>0.426019</td>\n",
       "      <td>0.009908</td>\n",
       "      <td>0.034878</td>\n",
       "      <td>3.060655</td>\n",
       "      <td>0.326727</td>\n",
       "      <td>2.060655</td>\n",
       "      <td>...</td>\n",
       "      <td>439.023968</td>\n",
       "      <td>5424.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>427.429572</td>\n",
       "      <td>0.078803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.870588</td>\n",
       "      <td>4.011726</td>\n",
       "      <td>0.078803</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_5</td>\n",
       "      <td>41274.125000</td>\n",
       "      <td>41439.0</td>\n",
       "      <td>14315.041992</td>\n",
       "      <td>0.346828</td>\n",
       "      <td>0.013596</td>\n",
       "      <td>0.065680</td>\n",
       "      <td>2.478506</td>\n",
       "      <td>0.403469</td>\n",
       "      <td>1.478506</td>\n",
       "      <td>...</td>\n",
       "      <td>485.209184</td>\n",
       "      <td>5096.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>726.731554</td>\n",
       "      <td>0.142608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.846909</td>\n",
       "      <td>3.571352</td>\n",
       "      <td>0.142608</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3240 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID     Feature_1  Feature_2     Feature_3  Feature_4  Feature_5  \\\n",
       "0  ID_1  18281.541667    18432.0   9409.650391   0.514708   0.011300   \n",
       "1  ID_2  20010.083333    20100.0   8303.049072   0.417707   0.014959   \n",
       "2  ID_3  27260.125000    27437.0  12189.649414   0.447160   0.011428   \n",
       "3  ID_4  41938.125000    42138.0  17866.433594   0.426019   0.009908   \n",
       "4  ID_5  41274.125000    41439.0  14315.041992   0.346828   0.013596   \n",
       "\n",
       "   Feature_6  Feature_7  Feature_8  Feature_9  ...  Feature_3230  \\\n",
       "0   0.045369   2.803803   0.356658   1.803803  ...    382.968383   \n",
       "1   0.080294   2.338398   0.429532   1.338398  ...    452.986164   \n",
       "2   0.046402   2.782842   0.359345   1.782842  ...    419.781765   \n",
       "3   0.034878   3.060655   0.326727   2.060655  ...    439.023968   \n",
       "4   0.065680   2.478506   0.403469   1.478506  ...    485.209184   \n",
       "\n",
       "   Feature_3231  Feature_3232  Feature_3233  Feature_3234  Feature_3235  \\\n",
       "0        2214.0           1.0    136.625113      0.061710           0.0   \n",
       "1        2548.5           1.0    232.564022      0.090548           0.0   \n",
       "2        3400.0           1.0    233.593529      0.068704           0.0   \n",
       "3        5424.0           1.0    427.429572      0.078803           0.0   \n",
       "4        5096.0           1.0    726.731554      0.142608           0.0   \n",
       "\n",
       "   Feature_3236  Feature_3237  Feature_3238  CLASS  \n",
       "0     28.154838      4.174959      0.061710      0  \n",
       "1     27.934229      3.931950      0.090548      1  \n",
       "2     27.904807      4.085035      0.068704      1  \n",
       "3     27.870588      4.011726      0.078803      0  \n",
       "4     28.846909      3.571352      0.142608      0  \n",
       "\n",
       "[5 rows x 3240 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7871080a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['CLASS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc45f8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop(['CLASS', 'ID'], axis=1, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "182109f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Quality Check:\n",
      "Infinite values: 4\n",
      "NaN values: 2668\n"
     ]
    }
   ],
   "source": [
    "# Check for infinite and extreme values first\n",
    "print(f\"\\nData Quality Check:\")\n",
    "inf_count = np.isinf(features.values).sum()\n",
    "nan_count = np.isnan(features.values).sum()\n",
    "print(f\"Infinite values: {inf_count}\")\n",
    "print(f\"NaN values: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68d54fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Feature_5</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_7</th>\n",
       "      <th>Feature_8</th>\n",
       "      <th>Feature_9</th>\n",
       "      <th>Feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature_3229</th>\n",
       "      <th>Feature_3230</th>\n",
       "      <th>Feature_3231</th>\n",
       "      <th>Feature_3232</th>\n",
       "      <th>Feature_3233</th>\n",
       "      <th>Feature_3234</th>\n",
       "      <th>Feature_3235</th>\n",
       "      <th>Feature_3236</th>\n",
       "      <th>Feature_3237</th>\n",
       "      <th>Feature_3238</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18281.541667</td>\n",
       "      <td>18432.0</td>\n",
       "      <td>9409.650391</td>\n",
       "      <td>0.514708</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.045369</td>\n",
       "      <td>2.803803</td>\n",
       "      <td>0.356658</td>\n",
       "      <td>1.803803</td>\n",
       "      <td>564.936250</td>\n",
       "      <td>...</td>\n",
       "      <td>382.968383</td>\n",
       "      <td>382.968383</td>\n",
       "      <td>2214.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>136.625113</td>\n",
       "      <td>0.061710</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.154838</td>\n",
       "      <td>4.174959</td>\n",
       "      <td>0.061710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20010.083333</td>\n",
       "      <td>20100.0</td>\n",
       "      <td>8303.049072</td>\n",
       "      <td>0.417707</td>\n",
       "      <td>0.014959</td>\n",
       "      <td>0.080294</td>\n",
       "      <td>2.338398</td>\n",
       "      <td>0.429532</td>\n",
       "      <td>1.338398</td>\n",
       "      <td>31.291507</td>\n",
       "      <td>...</td>\n",
       "      <td>452.986164</td>\n",
       "      <td>452.986164</td>\n",
       "      <td>2548.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>232.564022</td>\n",
       "      <td>0.090548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.934229</td>\n",
       "      <td>3.931950</td>\n",
       "      <td>0.090548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27260.125000</td>\n",
       "      <td>27437.0</td>\n",
       "      <td>12189.649414</td>\n",
       "      <td>0.447160</td>\n",
       "      <td>0.011428</td>\n",
       "      <td>0.046402</td>\n",
       "      <td>2.782842</td>\n",
       "      <td>0.359345</td>\n",
       "      <td>1.782842</td>\n",
       "      <td>11.965643</td>\n",
       "      <td>...</td>\n",
       "      <td>419.781765</td>\n",
       "      <td>419.781765</td>\n",
       "      <td>3400.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>233.593529</td>\n",
       "      <td>0.068704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.904807</td>\n",
       "      <td>4.085035</td>\n",
       "      <td>0.068704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41938.125000</td>\n",
       "      <td>42138.0</td>\n",
       "      <td>17866.433594</td>\n",
       "      <td>0.426019</td>\n",
       "      <td>0.009908</td>\n",
       "      <td>0.034878</td>\n",
       "      <td>3.060655</td>\n",
       "      <td>0.326727</td>\n",
       "      <td>2.060655</td>\n",
       "      <td>8.966286</td>\n",
       "      <td>...</td>\n",
       "      <td>439.023968</td>\n",
       "      <td>439.023968</td>\n",
       "      <td>5424.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>427.429572</td>\n",
       "      <td>0.078803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.870588</td>\n",
       "      <td>4.011726</td>\n",
       "      <td>0.078803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41274.125000</td>\n",
       "      <td>41439.0</td>\n",
       "      <td>14315.041992</td>\n",
       "      <td>0.346828</td>\n",
       "      <td>0.013596</td>\n",
       "      <td>0.065680</td>\n",
       "      <td>2.478506</td>\n",
       "      <td>0.403469</td>\n",
       "      <td>1.478506</td>\n",
       "      <td>34.898671</td>\n",
       "      <td>...</td>\n",
       "      <td>485.209184</td>\n",
       "      <td>485.209184</td>\n",
       "      <td>5096.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>726.731554</td>\n",
       "      <td>0.142608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.846909</td>\n",
       "      <td>3.571352</td>\n",
       "      <td>0.142608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3238 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Feature_1  Feature_2     Feature_3  Feature_4  Feature_5  Feature_6  \\\n",
       "0  18281.541667    18432.0   9409.650391   0.514708   0.011300   0.045369   \n",
       "1  20010.083333    20100.0   8303.049072   0.417707   0.014959   0.080294   \n",
       "2  27260.125000    27437.0  12189.649414   0.447160   0.011428   0.046402   \n",
       "3  41938.125000    42138.0  17866.433594   0.426019   0.009908   0.034878   \n",
       "4  41274.125000    41439.0  14315.041992   0.346828   0.013596   0.065680   \n",
       "\n",
       "   Feature_7  Feature_8  Feature_9  Feature_10  ...  Feature_3229  \\\n",
       "0   2.803803   0.356658   1.803803  564.936250  ...    382.968383   \n",
       "1   2.338398   0.429532   1.338398   31.291507  ...    452.986164   \n",
       "2   2.782842   0.359345   1.782842   11.965643  ...    419.781765   \n",
       "3   3.060655   0.326727   2.060655    8.966286  ...    439.023968   \n",
       "4   2.478506   0.403469   1.478506   34.898671  ...    485.209184   \n",
       "\n",
       "   Feature_3230  Feature_3231  Feature_3232  Feature_3233  Feature_3234  \\\n",
       "0    382.968383        2214.0           1.0    136.625113      0.061710   \n",
       "1    452.986164        2548.5           1.0    232.564022      0.090548   \n",
       "2    419.781765        3400.0           1.0    233.593529      0.068704   \n",
       "3    439.023968        5424.0           1.0    427.429572      0.078803   \n",
       "4    485.209184        5096.0           1.0    726.731554      0.142608   \n",
       "\n",
       "   Feature_3235  Feature_3236  Feature_3237  Feature_3238  \n",
       "0           0.0     28.154838      4.174959      0.061710  \n",
       "1           0.0     27.934229      3.931950      0.090548  \n",
       "2           0.0     27.904807      4.085035      0.068704  \n",
       "3           0.0     27.870588      4.011726      0.078803  \n",
       "4           0.0     28.846909      3.571352      0.142608  \n",
       "\n",
       "[5 rows x 3238 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace infinite values with NaN for statistics calculation\n",
    "features_clean = features.replace([np.inf, -np.inf], np.nan)\n",
    "features_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "281d02b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Feature_5</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_7</th>\n",
       "      <th>Feature_8</th>\n",
       "      <th>Feature_9</th>\n",
       "      <th>Feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature_3229</th>\n",
       "      <th>Feature_3230</th>\n",
       "      <th>Feature_3231</th>\n",
       "      <th>Feature_3232</th>\n",
       "      <th>Feature_3233</th>\n",
       "      <th>Feature_3234</th>\n",
       "      <th>Feature_3235</th>\n",
       "      <th>Feature_3236</th>\n",
       "      <th>Feature_3237</th>\n",
       "      <th>Feature_3238</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18281.541667</td>\n",
       "      <td>18432.0</td>\n",
       "      <td>9409.650391</td>\n",
       "      <td>0.514708</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.045369</td>\n",
       "      <td>2.803803</td>\n",
       "      <td>0.356658</td>\n",
       "      <td>1.803803</td>\n",
       "      <td>564.936250</td>\n",
       "      <td>...</td>\n",
       "      <td>382.968383</td>\n",
       "      <td>382.968383</td>\n",
       "      <td>2214.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>136.625113</td>\n",
       "      <td>0.061710</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.154838</td>\n",
       "      <td>4.174959</td>\n",
       "      <td>0.061710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20010.083333</td>\n",
       "      <td>20100.0</td>\n",
       "      <td>8303.049072</td>\n",
       "      <td>0.417707</td>\n",
       "      <td>0.014959</td>\n",
       "      <td>0.080294</td>\n",
       "      <td>2.338398</td>\n",
       "      <td>0.429532</td>\n",
       "      <td>1.338398</td>\n",
       "      <td>31.291507</td>\n",
       "      <td>...</td>\n",
       "      <td>452.986164</td>\n",
       "      <td>452.986164</td>\n",
       "      <td>2548.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>232.564022</td>\n",
       "      <td>0.090548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.934229</td>\n",
       "      <td>3.931950</td>\n",
       "      <td>0.090548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27260.125000</td>\n",
       "      <td>27437.0</td>\n",
       "      <td>12189.649414</td>\n",
       "      <td>0.447160</td>\n",
       "      <td>0.011428</td>\n",
       "      <td>0.046402</td>\n",
       "      <td>2.782842</td>\n",
       "      <td>0.359345</td>\n",
       "      <td>1.782842</td>\n",
       "      <td>11.965643</td>\n",
       "      <td>...</td>\n",
       "      <td>419.781765</td>\n",
       "      <td>419.781765</td>\n",
       "      <td>3400.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>233.593529</td>\n",
       "      <td>0.068704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.904807</td>\n",
       "      <td>4.085035</td>\n",
       "      <td>0.068704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41938.125000</td>\n",
       "      <td>42138.0</td>\n",
       "      <td>17866.433594</td>\n",
       "      <td>0.426019</td>\n",
       "      <td>0.009908</td>\n",
       "      <td>0.034878</td>\n",
       "      <td>3.060655</td>\n",
       "      <td>0.326727</td>\n",
       "      <td>2.060655</td>\n",
       "      <td>8.966286</td>\n",
       "      <td>...</td>\n",
       "      <td>439.023968</td>\n",
       "      <td>439.023968</td>\n",
       "      <td>5424.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>427.429572</td>\n",
       "      <td>0.078803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.870588</td>\n",
       "      <td>4.011726</td>\n",
       "      <td>0.078803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41274.125000</td>\n",
       "      <td>41439.0</td>\n",
       "      <td>14315.041992</td>\n",
       "      <td>0.346828</td>\n",
       "      <td>0.013596</td>\n",
       "      <td>0.065680</td>\n",
       "      <td>2.478506</td>\n",
       "      <td>0.403469</td>\n",
       "      <td>1.478506</td>\n",
       "      <td>34.898671</td>\n",
       "      <td>...</td>\n",
       "      <td>485.209184</td>\n",
       "      <td>485.209184</td>\n",
       "      <td>5096.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>726.731554</td>\n",
       "      <td>0.142608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.846909</td>\n",
       "      <td>3.571352</td>\n",
       "      <td>0.142608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>46787.916667</td>\n",
       "      <td>47002.0</td>\n",
       "      <td>18052.070312</td>\n",
       "      <td>0.385828</td>\n",
       "      <td>0.010883</td>\n",
       "      <td>0.042086</td>\n",
       "      <td>2.874885</td>\n",
       "      <td>0.347840</td>\n",
       "      <td>1.874885</td>\n",
       "      <td>23.499143</td>\n",
       "      <td>...</td>\n",
       "      <td>466.276055</td>\n",
       "      <td>466.276055</td>\n",
       "      <td>6064.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>585.547823</td>\n",
       "      <td>0.096561</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.787507</td>\n",
       "      <td>3.894684</td>\n",
       "      <td>0.096561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>8420.354167</td>\n",
       "      <td>8493.0</td>\n",
       "      <td>4292.039795</td>\n",
       "      <td>0.510004</td>\n",
       "      <td>0.016911</td>\n",
       "      <td>0.101797</td>\n",
       "      <td>2.145061</td>\n",
       "      <td>0.466555</td>\n",
       "      <td>1.145061</td>\n",
       "      <td>25.293867</td>\n",
       "      <td>...</td>\n",
       "      <td>383.044821</td>\n",
       "      <td>383.044821</td>\n",
       "      <td>987.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>69.155790</td>\n",
       "      <td>0.070369</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.625473</td>\n",
       "      <td>4.098452</td>\n",
       "      <td>0.070369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>37262.750000</td>\n",
       "      <td>37407.0</td>\n",
       "      <td>13950.793945</td>\n",
       "      <td>0.374390</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>0.057837</td>\n",
       "      <td>2.585819</td>\n",
       "      <td>0.386725</td>\n",
       "      <td>1.585819</td>\n",
       "      <td>37.474634</td>\n",
       "      <td>...</td>\n",
       "      <td>469.005263</td>\n",
       "      <td>469.005263</td>\n",
       "      <td>4940.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>505.566802</td>\n",
       "      <td>0.102341</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.865256</td>\n",
       "      <td>3.815115</td>\n",
       "      <td>0.102341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>25081.833333</td>\n",
       "      <td>25251.0</td>\n",
       "      <td>11689.275391</td>\n",
       "      <td>0.466045</td>\n",
       "      <td>0.011197</td>\n",
       "      <td>0.044546</td>\n",
       "      <td>2.820962</td>\n",
       "      <td>0.354489</td>\n",
       "      <td>1.820962</td>\n",
       "      <td>18.321132</td>\n",
       "      <td>...</td>\n",
       "      <td>403.597826</td>\n",
       "      <td>403.597826</td>\n",
       "      <td>2944.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>215.172554</td>\n",
       "      <td>0.073089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.180584</td>\n",
       "      <td>4.055504</td>\n",
       "      <td>0.073089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>52925.083333</td>\n",
       "      <td>53137.0</td>\n",
       "      <td>19790.341797</td>\n",
       "      <td>0.373931</td>\n",
       "      <td>0.010725</td>\n",
       "      <td>0.040871</td>\n",
       "      <td>2.903096</td>\n",
       "      <td>0.344460</td>\n",
       "      <td>1.903096</td>\n",
       "      <td>35.784277</td>\n",
       "      <td>...</td>\n",
       "      <td>479.596830</td>\n",
       "      <td>479.596830</td>\n",
       "      <td>6625.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>789.191094</td>\n",
       "      <td>0.119123</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.689509</td>\n",
       "      <td>3.765621</td>\n",
       "      <td>0.119123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>315 rows Ã— 3215 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Feature_1  Feature_2     Feature_3  Feature_4  Feature_5  Feature_6  \\\n",
       "0    18281.541667    18432.0   9409.650391   0.514708   0.011300   0.045369   \n",
       "1    20010.083333    20100.0   8303.049072   0.417707   0.014959   0.080294   \n",
       "2    27260.125000    27437.0  12189.649414   0.447160   0.011428   0.046402   \n",
       "3    41938.125000    42138.0  17866.433594   0.426019   0.009908   0.034878   \n",
       "4    41274.125000    41439.0  14315.041992   0.346828   0.013596   0.065680   \n",
       "..            ...        ...           ...        ...        ...        ...   \n",
       "310  46787.916667    47002.0  18052.070312   0.385828   0.010883   0.042086   \n",
       "311   8420.354167     8493.0   4292.039795   0.510004   0.016911   0.101797   \n",
       "312  37262.750000    37407.0  13950.793945   0.374390   0.012759   0.057837   \n",
       "313  25081.833333    25251.0  11689.275391   0.466045   0.011197   0.044546   \n",
       "314  52925.083333    53137.0  19790.341797   0.373931   0.010725   0.040871   \n",
       "\n",
       "     Feature_7  Feature_8  Feature_9  Feature_10  ...  Feature_3229  \\\n",
       "0     2.803803   0.356658   1.803803  564.936250  ...    382.968383   \n",
       "1     2.338398   0.429532   1.338398   31.291507  ...    452.986164   \n",
       "2     2.782842   0.359345   1.782842   11.965643  ...    419.781765   \n",
       "3     3.060655   0.326727   2.060655    8.966286  ...    439.023968   \n",
       "4     2.478506   0.403469   1.478506   34.898671  ...    485.209184   \n",
       "..         ...        ...        ...         ...  ...           ...   \n",
       "310   2.874885   0.347840   1.874885   23.499143  ...    466.276055   \n",
       "311   2.145061   0.466555   1.145061   25.293867  ...    383.044821   \n",
       "312   2.585819   0.386725   1.585819   37.474634  ...    469.005263   \n",
       "313   2.820962   0.354489   1.820962   18.321132  ...    403.597826   \n",
       "314   2.903096   0.344460   1.903096   35.784277  ...    479.596830   \n",
       "\n",
       "     Feature_3230  Feature_3231  Feature_3232  Feature_3233  Feature_3234  \\\n",
       "0      382.968383        2214.0           1.0    136.625113      0.061710   \n",
       "1      452.986164        2548.5           1.0    232.564022      0.090548   \n",
       "2      419.781765        3400.0           1.0    233.593529      0.068704   \n",
       "3      439.023968        5424.0           1.0    427.429572      0.078803   \n",
       "4      485.209184        5096.0           1.0    726.731554      0.142608   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "310    466.276055        6064.0           1.0    585.547823      0.096561   \n",
       "311    383.044821         987.0           1.0     69.155790      0.070369   \n",
       "312    469.005263        4940.0           1.0    505.566802      0.102341   \n",
       "313    403.597826        2944.0           1.0    215.172554      0.073089   \n",
       "314    479.596830        6625.0           1.0    789.191094      0.119123   \n",
       "\n",
       "     Feature_3235  Feature_3236  Feature_3237  Feature_3238  \n",
       "0             0.0     28.154838      4.174959      0.061710  \n",
       "1             0.0     27.934229      3.931950      0.090548  \n",
       "2             0.0     27.904807      4.085035      0.068704  \n",
       "3             0.0     27.870588      4.011726      0.078803  \n",
       "4             0.0     28.846909      3.571352      0.142608  \n",
       "..            ...           ...           ...           ...  \n",
       "310           0.0     28.787507      3.894684      0.096561  \n",
       "311           0.0     29.625473      4.098452      0.070369  \n",
       "312           0.0     26.865256      3.815115      0.102341  \n",
       "313           0.0     29.180584      4.055504      0.073089  \n",
       "314           0.0     29.689509      3.765621      0.119123  \n",
       "\n",
       "[315 rows x 3215 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_clean.dropna(axis=1, thresh=0.9 * len(features_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be0b55b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute remaining NaNs with column mean\n",
    "features_clean.fillna(features_clean.mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4166ffdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. REMOVE ZERO-VARIANCE FEATURES\n",
    "var_thresh = VarianceThreshold(threshold=0.0)\n",
    "X_var = var_thresh.fit_transform(features_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8252e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE HIGHLY CORRELATED FEATURES\n",
    "def remove_high_correlation_features(X, threshold=0.95):\n",
    "    corr_matrix = pd.DataFrame(X).corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    return pd.DataFrame(X).drop(columns=to_drop, axis=1)\n",
    "\n",
    "X_clean = remove_high_correlation_features(X_var, threshold=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc9e6b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>...</th>\n",
       "      <th>3079</th>\n",
       "      <th>3080</th>\n",
       "      <th>3084</th>\n",
       "      <th>3085</th>\n",
       "      <th>3093</th>\n",
       "      <th>3096</th>\n",
       "      <th>3100</th>\n",
       "      <th>3109</th>\n",
       "      <th>3112</th>\n",
       "      <th>3121</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18281.541667</td>\n",
       "      <td>9409.650391</td>\n",
       "      <td>0.514708</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>564.936250</td>\n",
       "      <td>179.125654</td>\n",
       "      <td>44.140704</td>\n",
       "      <td>18.149861</td>\n",
       "      <td>0.216274</td>\n",
       "      <td>0.088928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020695</td>\n",
       "      <td>0.005768</td>\n",
       "      <td>0.004403</td>\n",
       "      <td>0.012050</td>\n",
       "      <td>32.099384</td>\n",
       "      <td>0.003989</td>\n",
       "      <td>0.003850</td>\n",
       "      <td>28.957580</td>\n",
       "      <td>0.003820</td>\n",
       "      <td>28.154838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20010.083333</td>\n",
       "      <td>8303.049072</td>\n",
       "      <td>0.417707</td>\n",
       "      <td>0.014959</td>\n",
       "      <td>31.291507</td>\n",
       "      <td>122.447882</td>\n",
       "      <td>27.150254</td>\n",
       "      <td>14.165947</td>\n",
       "      <td>0.207880</td>\n",
       "      <td>0.108412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052191</td>\n",
       "      <td>0.003585</td>\n",
       "      <td>0.002466</td>\n",
       "      <td>0.009676</td>\n",
       "      <td>30.100056</td>\n",
       "      <td>0.003342</td>\n",
       "      <td>0.003069</td>\n",
       "      <td>28.665010</td>\n",
       "      <td>0.003124</td>\n",
       "      <td>27.934229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27260.125000</td>\n",
       "      <td>12189.649414</td>\n",
       "      <td>0.447160</td>\n",
       "      <td>0.011428</td>\n",
       "      <td>11.965643</td>\n",
       "      <td>241.904940</td>\n",
       "      <td>47.572298</td>\n",
       "      <td>22.613842</td>\n",
       "      <td>0.161702</td>\n",
       "      <td>0.076866</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022511</td>\n",
       "      <td>0.003818</td>\n",
       "      <td>0.002143</td>\n",
       "      <td>0.016889</td>\n",
       "      <td>30.148227</td>\n",
       "      <td>0.003562</td>\n",
       "      <td>0.002487</td>\n",
       "      <td>28.948552</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>27.904807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41938.125000</td>\n",
       "      <td>17866.433594</td>\n",
       "      <td>0.426019</td>\n",
       "      <td>0.009908</td>\n",
       "      <td>8.966286</td>\n",
       "      <td>226.260911</td>\n",
       "      <td>34.835854</td>\n",
       "      <td>29.188142</td>\n",
       "      <td>0.135663</td>\n",
       "      <td>0.113669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022961</td>\n",
       "      <td>0.004408</td>\n",
       "      <td>0.002557</td>\n",
       "      <td>0.017470</td>\n",
       "      <td>30.898766</td>\n",
       "      <td>0.003814</td>\n",
       "      <td>0.002860</td>\n",
       "      <td>29.198077</td>\n",
       "      <td>0.003292</td>\n",
       "      <td>27.870588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41274.125000</td>\n",
       "      <td>14315.041992</td>\n",
       "      <td>0.346828</td>\n",
       "      <td>0.013596</td>\n",
       "      <td>34.898671</td>\n",
       "      <td>208.619270</td>\n",
       "      <td>45.788460</td>\n",
       "      <td>20.180269</td>\n",
       "      <td>0.175847</td>\n",
       "      <td>0.077501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045295</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.001756</td>\n",
       "      <td>0.013465</td>\n",
       "      <td>31.877879</td>\n",
       "      <td>0.003408</td>\n",
       "      <td>0.002936</td>\n",
       "      <td>31.136644</td>\n",
       "      <td>0.002901</td>\n",
       "      <td>28.846909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>46787.916667</td>\n",
       "      <td>18052.070312</td>\n",
       "      <td>0.385828</td>\n",
       "      <td>0.010883</td>\n",
       "      <td>23.499143</td>\n",
       "      <td>278.217541</td>\n",
       "      <td>48.569993</td>\n",
       "      <td>21.849623</td>\n",
       "      <td>0.149795</td>\n",
       "      <td>0.067387</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039386</td>\n",
       "      <td>0.003363</td>\n",
       "      <td>0.002989</td>\n",
       "      <td>0.004861</td>\n",
       "      <td>30.297602</td>\n",
       "      <td>0.003033</td>\n",
       "      <td>0.003033</td>\n",
       "      <td>28.787507</td>\n",
       "      <td>0.003033</td>\n",
       "      <td>28.787507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>8420.354167</td>\n",
       "      <td>4292.039795</td>\n",
       "      <td>0.510004</td>\n",
       "      <td>0.016911</td>\n",
       "      <td>25.293867</td>\n",
       "      <td>93.618836</td>\n",
       "      <td>18.100903</td>\n",
       "      <td>13.007549</td>\n",
       "      <td>0.181568</td>\n",
       "      <td>0.131103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028983</td>\n",
       "      <td>0.006227</td>\n",
       "      <td>0.003642</td>\n",
       "      <td>0.023425</td>\n",
       "      <td>32.628572</td>\n",
       "      <td>0.005473</td>\n",
       "      <td>0.004316</td>\n",
       "      <td>30.582970</td>\n",
       "      <td>0.003948</td>\n",
       "      <td>29.625473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>37262.750000</td>\n",
       "      <td>13950.793945</td>\n",
       "      <td>0.374390</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>37.474634</td>\n",
       "      <td>202.610464</td>\n",
       "      <td>38.141866</td>\n",
       "      <td>28.135131</td>\n",
       "      <td>0.156558</td>\n",
       "      <td>0.115484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022950</td>\n",
       "      <td>0.003339</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.009559</td>\n",
       "      <td>29.512201</td>\n",
       "      <td>0.002889</td>\n",
       "      <td>0.002889</td>\n",
       "      <td>26.865256</td>\n",
       "      <td>0.002889</td>\n",
       "      <td>26.865256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>25081.833333</td>\n",
       "      <td>11689.275391</td>\n",
       "      <td>0.466045</td>\n",
       "      <td>0.011197</td>\n",
       "      <td>18.321132</td>\n",
       "      <td>204.320826</td>\n",
       "      <td>47.209078</td>\n",
       "      <td>25.232333</td>\n",
       "      <td>0.194147</td>\n",
       "      <td>0.103768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063719</td>\n",
       "      <td>0.003918</td>\n",
       "      <td>0.003684</td>\n",
       "      <td>0.004855</td>\n",
       "      <td>31.186746</td>\n",
       "      <td>0.003616</td>\n",
       "      <td>0.003616</td>\n",
       "      <td>29.180584</td>\n",
       "      <td>0.003616</td>\n",
       "      <td>29.180584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>52925.083333</td>\n",
       "      <td>19790.341797</td>\n",
       "      <td>0.373931</td>\n",
       "      <td>0.010725</td>\n",
       "      <td>35.784277</td>\n",
       "      <td>215.596846</td>\n",
       "      <td>52.378874</td>\n",
       "      <td>31.852198</td>\n",
       "      <td>0.206894</td>\n",
       "      <td>0.125815</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043348</td>\n",
       "      <td>0.003459</td>\n",
       "      <td>0.002067</td>\n",
       "      <td>0.013313</td>\n",
       "      <td>31.767568</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>0.002678</td>\n",
       "      <td>31.410935</td>\n",
       "      <td>0.003111</td>\n",
       "      <td>29.689509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>315 rows Ã— 721 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0             2         3         4           9           10    \\\n",
       "0    18281.541667   9409.650391  0.514708  0.011300  564.936250  179.125654   \n",
       "1    20010.083333   8303.049072  0.417707  0.014959   31.291507  122.447882   \n",
       "2    27260.125000  12189.649414  0.447160  0.011428   11.965643  241.904940   \n",
       "3    41938.125000  17866.433594  0.426019  0.009908    8.966286  226.260911   \n",
       "4    41274.125000  14315.041992  0.346828  0.013596   34.898671  208.619270   \n",
       "..            ...           ...       ...       ...         ...         ...   \n",
       "310  46787.916667  18052.070312  0.385828  0.010883   23.499143  278.217541   \n",
       "311   8420.354167   4292.039795  0.510004  0.016911   25.293867   93.618836   \n",
       "312  37262.750000  13950.793945  0.374390  0.012759   37.474634  202.610464   \n",
       "313  25081.833333  11689.275391  0.466045  0.011197   18.321132  204.320826   \n",
       "314  52925.083333  19790.341797  0.373931  0.010725   35.784277  215.596846   \n",
       "\n",
       "          12         13        14        15    ...      3079      3080  \\\n",
       "0    44.140704  18.149861  0.216274  0.088928  ...  0.020695  0.005768   \n",
       "1    27.150254  14.165947  0.207880  0.108412  ...  0.052191  0.003585   \n",
       "2    47.572298  22.613842  0.161702  0.076866  ...  0.022511  0.003818   \n",
       "3    34.835854  29.188142  0.135663  0.113669  ...  0.022961  0.004408   \n",
       "4    45.788460  20.180269  0.175847  0.077501  ...  0.045295  0.003538   \n",
       "..         ...        ...       ...       ...  ...       ...       ...   \n",
       "310  48.569993  21.849623  0.149795  0.067387  ...  0.039386  0.003363   \n",
       "311  18.100903  13.007549  0.181568  0.131103  ...  0.028983  0.006227   \n",
       "312  38.141866  28.135131  0.156558  0.115484  ...  0.022950  0.003339   \n",
       "313  47.209078  25.232333  0.194147  0.103768  ...  0.063719  0.003918   \n",
       "314  52.378874  31.852198  0.206894  0.125815  ...  0.043348  0.003459   \n",
       "\n",
       "         3084      3085       3093      3096      3100       3109      3112  \\\n",
       "0    0.004403  0.012050  32.099384  0.003989  0.003850  28.957580  0.003820   \n",
       "1    0.002466  0.009676  30.100056  0.003342  0.003069  28.665010  0.003124   \n",
       "2    0.002143  0.016889  30.148227  0.003562  0.002487  28.948552  0.003357   \n",
       "3    0.002557  0.017470  30.898766  0.003814  0.002860  29.198077  0.003292   \n",
       "4    0.001756  0.013465  31.877879  0.003408  0.002936  31.136644  0.002901   \n",
       "..        ...       ...        ...       ...       ...        ...       ...   \n",
       "310  0.002989  0.004861  30.297602  0.003033  0.003033  28.787507  0.003033   \n",
       "311  0.003642  0.023425  32.628572  0.005473  0.004316  30.582970  0.003948   \n",
       "312  0.001783  0.009559  29.512201  0.002889  0.002889  26.865256  0.002889   \n",
       "313  0.003684  0.004855  31.186746  0.003616  0.003616  29.180584  0.003616   \n",
       "314  0.002067  0.013313  31.767568  0.003401  0.002678  31.410935  0.003111   \n",
       "\n",
       "          3121  \n",
       "0    28.154838  \n",
       "1    27.934229  \n",
       "2    27.904807  \n",
       "3    27.870588  \n",
       "4    28.846909  \n",
       "..         ...  \n",
       "310  28.787507  \n",
       "311  29.625473  \n",
       "312  26.865256  \n",
       "313  29.180584  \n",
       "314  29.689509  \n",
       "\n",
       "[315 rows x 721 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65a8441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. SCALING\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_var)  # or X_clean if using correlation pruning\n",
    "\n",
    "# 6. DIMENSIONALITY REDUCTION WITH PCA (retain 95% variance)\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7398e3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.01479477e+01, -1.17756834e+01, -7.11444547e+00, ...,\n",
       "        -4.62133631e-01, -2.60921423e-01, -7.61432107e-01],\n",
       "       [-3.23501158e+01,  3.06485465e+00, -1.37169466e+01, ...,\n",
       "        -9.87636086e-01, -1.02425093e+00,  1.66468832e+00],\n",
       "       [-2.35368430e+01, -2.09286818e+01,  5.40540931e+00, ...,\n",
       "         8.14779761e-02, -1.90705789e+00,  3.47616810e-01],\n",
       "       ...,\n",
       "       [ 3.67963762e+01, -2.05021567e+00,  4.56838899e+00, ...,\n",
       "         4.32130452e-01, -3.61061448e+00, -8.26717205e-01],\n",
       "       [-3.87449265e+01,  3.32913801e+01, -2.31169834e+01, ...,\n",
       "        -4.61947086e-01, -4.02150609e-01,  8.18816893e-01],\n",
       "       [ 2.34850075e+01,  3.43519623e+00,  1.17999404e+01, ...,\n",
       "         1.85065851e+00, -1.64494636e+00,  1.02091711e-02]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7eba600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      1\n",
       "2      1\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "310    0\n",
       "311    1\n",
       "312    0\n",
       "313    1\n",
       "314    0\n",
       "Name: CLASS, Length: 315, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=df['CLASS']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57e497b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LogisticRegression\n",
      " Fold 1: Accuracy: 0.5238, F1: 0.4828, Recall: 0.5833, Specificity: 0.4872, AUROC: 0.5801\n",
      " Fold 2: Accuracy: 0.5714, F1: 0.5714, Recall: 0.7200, Specificity: 0.4737, AUROC: 0.6379\n",
      " Fold 3: Accuracy: 0.6508, F1: 0.6071, Recall: 0.6800, Specificity: 0.6316, AUROC: 0.7168\n",
      " Fold 4: Accuracy: 0.6508, F1: 0.6207, Recall: 0.7200, Specificity: 0.6053, AUROC: 0.6547\n",
      " Fold 5: Accuracy: 0.6032, F1: 0.5763, Recall: 0.6800, Specificity: 0.5526, AUROC: 0.6621\n",
      "\n",
      "Training RandomForest\n",
      " Fold 1: Accuracy: 0.6349, F1: 0.3784, Recall: 0.2917, Specificity: 0.8462, AUROC: 0.6250\n",
      " Fold 2: Accuracy: 0.6032, F1: 0.4186, Recall: 0.3600, Specificity: 0.7632, AUROC: 0.6042\n",
      " Fold 3: Accuracy: 0.6032, F1: 0.2857, Recall: 0.2000, Specificity: 0.8684, AUROC: 0.5474\n",
      " Fold 4: Accuracy: 0.5556, F1: 0.1765, Recall: 0.1200, Specificity: 0.8421, AUROC: 0.5463\n",
      " Fold 5: Accuracy: 0.5079, F1: 0.2051, Recall: 0.1600, Specificity: 0.7368, AUROC: 0.4926\n",
      "\n",
      "Training XGBoost\n",
      " Fold 1: Accuracy: 0.5714, F1: 0.4255, Recall: 0.4167, Specificity: 0.6667, AUROC: 0.6154\n",
      " Fold 2: Accuracy: 0.6032, F1: 0.4681, Recall: 0.4400, Specificity: 0.7105, AUROC: 0.6126\n",
      " Fold 3: Accuracy: 0.5556, F1: 0.5000, Recall: 0.5600, Specificity: 0.5526, AUROC: 0.5368\n",
      " Fold 4: Accuracy: 0.5079, F1: 0.2051, Recall: 0.1600, Specificity: 0.7368, AUROC: 0.5558\n",
      " Fold 5: Accuracy: 0.4444, F1: 0.3137, Recall: 0.3200, Specificity: 0.5263, AUROC: 0.4347\n",
      "\n",
      "ðŸ“Š Average Cross-Validation Results:\n",
      " LogisticRegression: Accuracy: 0.6000, F1: 0.5717, Recall: 0.6767, Specificity: 0.5501, AUROC: 0.6503\n",
      " RandomForest: Accuracy: 0.5810, F1: 0.2929, Recall: 0.2263, Specificity: 0.8113, AUROC: 0.5631\n",
      " XGBoost: Accuracy: 0.5365, F1: 0.3825, Recall: 0.3793, Specificity: 0.6386, AUROC: 0.5511\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, recall_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Helper function to calculate all metrics\n",
    "def evaluate(y_true, y_pred, y_proba):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    return {\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'F1': f1_score(y_true, y_pred),\n",
    "        'Recall': recall_score(y_true, y_pred),\n",
    "        'Specificity': specificity,\n",
    "        'AUROC': roc_auc_score(y_true, y_proba)\n",
    "    }\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42,C=0.01,penalty= 'l1', solver= 'liblinear'),\n",
    "    \"RandomForest\": RandomForestClassifier(class_weight='balanced', n_estimators=100, random_state=42,max_depth=None,min_samples_leaf=2, min_samples_split=5),\n",
    "    \"XGBoost\": XGBClassifier(scale_pos_weight=191/124, eval_metric='logloss', random_state=42,learing_rate=0.2, n_estimators=200, max_depth=3, subsample=0.8, colsample_bytree=0.8, use_label_encoder=False, verbosity=0)\n",
    "}\n",
    "\n",
    "# Cross-validation setup\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = {model_name: [] for model_name in models}\n",
    "\n",
    "# Loop through models and perform cross-validation\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name}\")\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_pca, y)):\n",
    "        X_train, X_val = X_pca[train_idx], X_pca[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        y_proba = model.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        metrics = evaluate(y_val, y_pred, y_proba)\n",
    "        results[model_name].append(metrics)\n",
    "        print(f\" Fold {fold+1}: \" + \", \".join([f\"{k}: {v:.4f}\" for k, v in metrics.items()]))\n",
    "\n",
    "# Average results\n",
    "print(\"\\nðŸ“Š Average Cross-Validation Results:\")\n",
    "for model_name, folds in results.items():\n",
    "    avg = {k: np.mean([fold[k] for fold in folds]) for k in folds[0]}\n",
    "    print(f\" {model_name}: \" + \", \".join([f\"{k}: {v:.4f}\" for k, v in avg.items()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "838cf032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Logistic Regression params: {'C': 0.01, 'penalty': 'l1', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg_grid = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'solver': ['liblinear']  # required for L1\n",
    "}\n",
    "\n",
    "log_reg = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "log_cv = GridSearchCV(log_reg, log_reg_grid, scoring='f1', cv=5)\n",
    "log_cv.fit(X_pca, y)\n",
    "\n",
    "print(\"Best Logistic Regression params:\", log_cv.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d56aeaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest params: {'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "rf_cv = GridSearchCV(rf, rf_grid, scoring='f1', cv=5)\n",
    "rf_cv.fit(X_pca, y)\n",
    "\n",
    "print(\"Best Random Forest params:\", rf_cv.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "584554b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:17:37] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:17:37] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:17:38] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:17:38] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:17:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:17:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:17:40] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:17:40] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:17:41] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:17:41] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:17:42] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:17:43] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:17:43] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:17:44] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:17:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:17:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:17:47] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:17:48] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:17:49] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:17:51] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:17:52] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:17:54] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:17:55] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:17:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:17:57] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:17:58] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:00] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:02] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:04] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:06] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:08] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:09] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:09] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:09] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:10] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:10] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:11] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:12] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:12] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:13] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:14] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:14] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:15] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:17] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:18] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:19] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:20] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:21] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:22] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:23] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:24] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:25] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:25] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:26] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:27] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:29] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:30] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:31] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:32] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:34] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:34] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:35] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:35] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:36] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:36] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:37] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:37] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:38] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:40] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:40] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:41] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:41] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:42] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:42] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:43] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:44] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:46] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:47] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:47] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:48] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:49] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:49] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:50] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:51] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:52] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:53] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:18:54] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost params: {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "xgb = XGBClassifier(scale_pos_weight=191/124, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "xgb_cv = GridSearchCV(xgb, xgb_grid, scoring='f1', cv=5)\n",
    "xgb_cv.fit(X_pca, y)\n",
    "\n",
    "print(\"Best XGBoost params:\", xgb_cv.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41242dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
