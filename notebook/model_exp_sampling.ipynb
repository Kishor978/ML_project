{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87aa2bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, recall_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f2ba6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('E:\\\\NAAMII\\\\Machine_learning\\\\dataset\\\\train_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeb63960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Feature_5</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_7</th>\n",
       "      <th>Feature_8</th>\n",
       "      <th>Feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature_3230</th>\n",
       "      <th>Feature_3231</th>\n",
       "      <th>Feature_3232</th>\n",
       "      <th>Feature_3233</th>\n",
       "      <th>Feature_3234</th>\n",
       "      <th>Feature_3235</th>\n",
       "      <th>Feature_3236</th>\n",
       "      <th>Feature_3237</th>\n",
       "      <th>Feature_3238</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_1</td>\n",
       "      <td>18281.541667</td>\n",
       "      <td>18432.0</td>\n",
       "      <td>9409.650391</td>\n",
       "      <td>0.514708</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.045369</td>\n",
       "      <td>2.803803</td>\n",
       "      <td>0.356658</td>\n",
       "      <td>1.803803</td>\n",
       "      <td>...</td>\n",
       "      <td>382.968383</td>\n",
       "      <td>2214.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>136.625113</td>\n",
       "      <td>0.061710</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.154838</td>\n",
       "      <td>4.174959</td>\n",
       "      <td>0.061710</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_2</td>\n",
       "      <td>20010.083333</td>\n",
       "      <td>20100.0</td>\n",
       "      <td>8303.049072</td>\n",
       "      <td>0.417707</td>\n",
       "      <td>0.014959</td>\n",
       "      <td>0.080294</td>\n",
       "      <td>2.338398</td>\n",
       "      <td>0.429532</td>\n",
       "      <td>1.338398</td>\n",
       "      <td>...</td>\n",
       "      <td>452.986164</td>\n",
       "      <td>2548.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>232.564022</td>\n",
       "      <td>0.090548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.934229</td>\n",
       "      <td>3.931950</td>\n",
       "      <td>0.090548</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_3</td>\n",
       "      <td>27260.125000</td>\n",
       "      <td>27437.0</td>\n",
       "      <td>12189.649414</td>\n",
       "      <td>0.447160</td>\n",
       "      <td>0.011428</td>\n",
       "      <td>0.046402</td>\n",
       "      <td>2.782842</td>\n",
       "      <td>0.359345</td>\n",
       "      <td>1.782842</td>\n",
       "      <td>...</td>\n",
       "      <td>419.781765</td>\n",
       "      <td>3400.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>233.593529</td>\n",
       "      <td>0.068704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.904807</td>\n",
       "      <td>4.085035</td>\n",
       "      <td>0.068704</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_4</td>\n",
       "      <td>41938.125000</td>\n",
       "      <td>42138.0</td>\n",
       "      <td>17866.433594</td>\n",
       "      <td>0.426019</td>\n",
       "      <td>0.009908</td>\n",
       "      <td>0.034878</td>\n",
       "      <td>3.060655</td>\n",
       "      <td>0.326727</td>\n",
       "      <td>2.060655</td>\n",
       "      <td>...</td>\n",
       "      <td>439.023968</td>\n",
       "      <td>5424.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>427.429572</td>\n",
       "      <td>0.078803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.870588</td>\n",
       "      <td>4.011726</td>\n",
       "      <td>0.078803</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_5</td>\n",
       "      <td>41274.125000</td>\n",
       "      <td>41439.0</td>\n",
       "      <td>14315.041992</td>\n",
       "      <td>0.346828</td>\n",
       "      <td>0.013596</td>\n",
       "      <td>0.065680</td>\n",
       "      <td>2.478506</td>\n",
       "      <td>0.403469</td>\n",
       "      <td>1.478506</td>\n",
       "      <td>...</td>\n",
       "      <td>485.209184</td>\n",
       "      <td>5096.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>726.731554</td>\n",
       "      <td>0.142608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.846909</td>\n",
       "      <td>3.571352</td>\n",
       "      <td>0.142608</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3240 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID     Feature_1  Feature_2     Feature_3  Feature_4  Feature_5  \\\n",
       "0  ID_1  18281.541667    18432.0   9409.650391   0.514708   0.011300   \n",
       "1  ID_2  20010.083333    20100.0   8303.049072   0.417707   0.014959   \n",
       "2  ID_3  27260.125000    27437.0  12189.649414   0.447160   0.011428   \n",
       "3  ID_4  41938.125000    42138.0  17866.433594   0.426019   0.009908   \n",
       "4  ID_5  41274.125000    41439.0  14315.041992   0.346828   0.013596   \n",
       "\n",
       "   Feature_6  Feature_7  Feature_8  Feature_9  ...  Feature_3230  \\\n",
       "0   0.045369   2.803803   0.356658   1.803803  ...    382.968383   \n",
       "1   0.080294   2.338398   0.429532   1.338398  ...    452.986164   \n",
       "2   0.046402   2.782842   0.359345   1.782842  ...    419.781765   \n",
       "3   0.034878   3.060655   0.326727   2.060655  ...    439.023968   \n",
       "4   0.065680   2.478506   0.403469   1.478506  ...    485.209184   \n",
       "\n",
       "   Feature_3231  Feature_3232  Feature_3233  Feature_3234  Feature_3235  \\\n",
       "0        2214.0           1.0    136.625113      0.061710           0.0   \n",
       "1        2548.5           1.0    232.564022      0.090548           0.0   \n",
       "2        3400.0           1.0    233.593529      0.068704           0.0   \n",
       "3        5424.0           1.0    427.429572      0.078803           0.0   \n",
       "4        5096.0           1.0    726.731554      0.142608           0.0   \n",
       "\n",
       "   Feature_3236  Feature_3237  Feature_3238  CLASS  \n",
       "0     28.154838      4.174959      0.061710      0  \n",
       "1     27.934229      3.931950      0.090548      1  \n",
       "2     27.904807      4.085035      0.068704      1  \n",
       "3     27.870588      4.011726      0.078803      0  \n",
       "4     28.846909      3.571352      0.142608      0  \n",
       "\n",
       "[5 rows x 3240 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e6c5865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Feature_5</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_7</th>\n",
       "      <th>Feature_8</th>\n",
       "      <th>Feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature_3230</th>\n",
       "      <th>Feature_3231</th>\n",
       "      <th>Feature_3232</th>\n",
       "      <th>Feature_3233</th>\n",
       "      <th>Feature_3234</th>\n",
       "      <th>Feature_3235</th>\n",
       "      <th>Feature_3236</th>\n",
       "      <th>Feature_3237</th>\n",
       "      <th>Feature_3238</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_1</td>\n",
       "      <td>18281.541667</td>\n",
       "      <td>18432.0</td>\n",
       "      <td>9409.650391</td>\n",
       "      <td>0.514708</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.045369</td>\n",
       "      <td>2.803803</td>\n",
       "      <td>0.356658</td>\n",
       "      <td>1.803803</td>\n",
       "      <td>...</td>\n",
       "      <td>382.968383</td>\n",
       "      <td>2214.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>136.625113</td>\n",
       "      <td>0.061710</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.154838</td>\n",
       "      <td>4.174959</td>\n",
       "      <td>0.061710</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_2</td>\n",
       "      <td>20010.083333</td>\n",
       "      <td>20100.0</td>\n",
       "      <td>8303.049072</td>\n",
       "      <td>0.417707</td>\n",
       "      <td>0.014959</td>\n",
       "      <td>0.080294</td>\n",
       "      <td>2.338398</td>\n",
       "      <td>0.429532</td>\n",
       "      <td>1.338398</td>\n",
       "      <td>...</td>\n",
       "      <td>452.986164</td>\n",
       "      <td>2548.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>232.564022</td>\n",
       "      <td>0.090548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.934229</td>\n",
       "      <td>3.931950</td>\n",
       "      <td>0.090548</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_3</td>\n",
       "      <td>27260.125000</td>\n",
       "      <td>27437.0</td>\n",
       "      <td>12189.649414</td>\n",
       "      <td>0.447160</td>\n",
       "      <td>0.011428</td>\n",
       "      <td>0.046402</td>\n",
       "      <td>2.782842</td>\n",
       "      <td>0.359345</td>\n",
       "      <td>1.782842</td>\n",
       "      <td>...</td>\n",
       "      <td>419.781765</td>\n",
       "      <td>3400.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>233.593529</td>\n",
       "      <td>0.068704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.904807</td>\n",
       "      <td>4.085035</td>\n",
       "      <td>0.068704</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_4</td>\n",
       "      <td>41938.125000</td>\n",
       "      <td>42138.0</td>\n",
       "      <td>17866.433594</td>\n",
       "      <td>0.426019</td>\n",
       "      <td>0.009908</td>\n",
       "      <td>0.034878</td>\n",
       "      <td>3.060655</td>\n",
       "      <td>0.326727</td>\n",
       "      <td>2.060655</td>\n",
       "      <td>...</td>\n",
       "      <td>439.023968</td>\n",
       "      <td>5424.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>427.429572</td>\n",
       "      <td>0.078803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.870588</td>\n",
       "      <td>4.011726</td>\n",
       "      <td>0.078803</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_5</td>\n",
       "      <td>41274.125000</td>\n",
       "      <td>41439.0</td>\n",
       "      <td>14315.041992</td>\n",
       "      <td>0.346828</td>\n",
       "      <td>0.013596</td>\n",
       "      <td>0.065680</td>\n",
       "      <td>2.478506</td>\n",
       "      <td>0.403469</td>\n",
       "      <td>1.478506</td>\n",
       "      <td>...</td>\n",
       "      <td>485.209184</td>\n",
       "      <td>5096.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>726.731554</td>\n",
       "      <td>0.142608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.846909</td>\n",
       "      <td>3.571352</td>\n",
       "      <td>0.142608</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3240 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID     Feature_1  Feature_2     Feature_3  Feature_4  Feature_5  \\\n",
       "0  ID_1  18281.541667    18432.0   9409.650391   0.514708   0.011300   \n",
       "1  ID_2  20010.083333    20100.0   8303.049072   0.417707   0.014959   \n",
       "2  ID_3  27260.125000    27437.0  12189.649414   0.447160   0.011428   \n",
       "3  ID_4  41938.125000    42138.0  17866.433594   0.426019   0.009908   \n",
       "4  ID_5  41274.125000    41439.0  14315.041992   0.346828   0.013596   \n",
       "\n",
       "   Feature_6  Feature_7  Feature_8  Feature_9  ...  Feature_3230  \\\n",
       "0   0.045369   2.803803   0.356658   1.803803  ...    382.968383   \n",
       "1   0.080294   2.338398   0.429532   1.338398  ...    452.986164   \n",
       "2   0.046402   2.782842   0.359345   1.782842  ...    419.781765   \n",
       "3   0.034878   3.060655   0.326727   2.060655  ...    439.023968   \n",
       "4   0.065680   2.478506   0.403469   1.478506  ...    485.209184   \n",
       "\n",
       "   Feature_3231  Feature_3232  Feature_3233  Feature_3234  Feature_3235  \\\n",
       "0        2214.0           1.0    136.625113      0.061710           0.0   \n",
       "1        2548.5           1.0    232.564022      0.090548           0.0   \n",
       "2        3400.0           1.0    233.593529      0.068704           0.0   \n",
       "3        5424.0           1.0    427.429572      0.078803           0.0   \n",
       "4        5096.0           1.0    726.731554      0.142608           0.0   \n",
       "\n",
       "   Feature_3236  Feature_3237  Feature_3238  CLASS  \n",
       "0     28.154838      4.174959      0.061710      0  \n",
       "1     27.934229      3.931950      0.090548      1  \n",
       "2     27.904807      4.085035      0.068704      1  \n",
       "3     27.870588      4.011726      0.078803      0  \n",
       "4     28.846909      3.571352      0.142608      0  \n",
       "\n",
       "[5 rows x 3240 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7871080a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      1\n",
       "2      1\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "310    0\n",
       "311    1\n",
       "312    0\n",
       "313    1\n",
       "314    0\n",
       "Name: CLASS, Length: 315, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=df['CLASS']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc45f8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop(['CLASS', 'ID'], axis=1, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "182109f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Quality Check:\n",
      "Infinite values: 4\n",
      "NaN values: 2668\n"
     ]
    }
   ],
   "source": [
    "# Check for infinite and extreme values first\n",
    "print(f\"\\nData Quality Check:\")\n",
    "inf_count = np.isinf(features.values).sum()\n",
    "nan_count = np.isnan(features.values).sum()\n",
    "print(f\"Infinite values: {inf_count}\")\n",
    "print(f\"NaN values: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68d54fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Feature_5</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_7</th>\n",
       "      <th>Feature_8</th>\n",
       "      <th>Feature_9</th>\n",
       "      <th>Feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature_3229</th>\n",
       "      <th>Feature_3230</th>\n",
       "      <th>Feature_3231</th>\n",
       "      <th>Feature_3232</th>\n",
       "      <th>Feature_3233</th>\n",
       "      <th>Feature_3234</th>\n",
       "      <th>Feature_3235</th>\n",
       "      <th>Feature_3236</th>\n",
       "      <th>Feature_3237</th>\n",
       "      <th>Feature_3238</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18281.541667</td>\n",
       "      <td>18432.0</td>\n",
       "      <td>9409.650391</td>\n",
       "      <td>0.514708</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.045369</td>\n",
       "      <td>2.803803</td>\n",
       "      <td>0.356658</td>\n",
       "      <td>1.803803</td>\n",
       "      <td>564.936250</td>\n",
       "      <td>...</td>\n",
       "      <td>382.968383</td>\n",
       "      <td>382.968383</td>\n",
       "      <td>2214.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>136.625113</td>\n",
       "      <td>0.061710</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.154838</td>\n",
       "      <td>4.174959</td>\n",
       "      <td>0.061710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20010.083333</td>\n",
       "      <td>20100.0</td>\n",
       "      <td>8303.049072</td>\n",
       "      <td>0.417707</td>\n",
       "      <td>0.014959</td>\n",
       "      <td>0.080294</td>\n",
       "      <td>2.338398</td>\n",
       "      <td>0.429532</td>\n",
       "      <td>1.338398</td>\n",
       "      <td>31.291507</td>\n",
       "      <td>...</td>\n",
       "      <td>452.986164</td>\n",
       "      <td>452.986164</td>\n",
       "      <td>2548.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>232.564022</td>\n",
       "      <td>0.090548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.934229</td>\n",
       "      <td>3.931950</td>\n",
       "      <td>0.090548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27260.125000</td>\n",
       "      <td>27437.0</td>\n",
       "      <td>12189.649414</td>\n",
       "      <td>0.447160</td>\n",
       "      <td>0.011428</td>\n",
       "      <td>0.046402</td>\n",
       "      <td>2.782842</td>\n",
       "      <td>0.359345</td>\n",
       "      <td>1.782842</td>\n",
       "      <td>11.965643</td>\n",
       "      <td>...</td>\n",
       "      <td>419.781765</td>\n",
       "      <td>419.781765</td>\n",
       "      <td>3400.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>233.593529</td>\n",
       "      <td>0.068704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.904807</td>\n",
       "      <td>4.085035</td>\n",
       "      <td>0.068704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41938.125000</td>\n",
       "      <td>42138.0</td>\n",
       "      <td>17866.433594</td>\n",
       "      <td>0.426019</td>\n",
       "      <td>0.009908</td>\n",
       "      <td>0.034878</td>\n",
       "      <td>3.060655</td>\n",
       "      <td>0.326727</td>\n",
       "      <td>2.060655</td>\n",
       "      <td>8.966286</td>\n",
       "      <td>...</td>\n",
       "      <td>439.023968</td>\n",
       "      <td>439.023968</td>\n",
       "      <td>5424.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>427.429572</td>\n",
       "      <td>0.078803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.870588</td>\n",
       "      <td>4.011726</td>\n",
       "      <td>0.078803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41274.125000</td>\n",
       "      <td>41439.0</td>\n",
       "      <td>14315.041992</td>\n",
       "      <td>0.346828</td>\n",
       "      <td>0.013596</td>\n",
       "      <td>0.065680</td>\n",
       "      <td>2.478506</td>\n",
       "      <td>0.403469</td>\n",
       "      <td>1.478506</td>\n",
       "      <td>34.898671</td>\n",
       "      <td>...</td>\n",
       "      <td>485.209184</td>\n",
       "      <td>485.209184</td>\n",
       "      <td>5096.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>726.731554</td>\n",
       "      <td>0.142608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.846909</td>\n",
       "      <td>3.571352</td>\n",
       "      <td>0.142608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3238 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Feature_1  Feature_2     Feature_3  Feature_4  Feature_5  Feature_6  \\\n",
       "0  18281.541667    18432.0   9409.650391   0.514708   0.011300   0.045369   \n",
       "1  20010.083333    20100.0   8303.049072   0.417707   0.014959   0.080294   \n",
       "2  27260.125000    27437.0  12189.649414   0.447160   0.011428   0.046402   \n",
       "3  41938.125000    42138.0  17866.433594   0.426019   0.009908   0.034878   \n",
       "4  41274.125000    41439.0  14315.041992   0.346828   0.013596   0.065680   \n",
       "\n",
       "   Feature_7  Feature_8  Feature_9  Feature_10  ...  Feature_3229  \\\n",
       "0   2.803803   0.356658   1.803803  564.936250  ...    382.968383   \n",
       "1   2.338398   0.429532   1.338398   31.291507  ...    452.986164   \n",
       "2   2.782842   0.359345   1.782842   11.965643  ...    419.781765   \n",
       "3   3.060655   0.326727   2.060655    8.966286  ...    439.023968   \n",
       "4   2.478506   0.403469   1.478506   34.898671  ...    485.209184   \n",
       "\n",
       "   Feature_3230  Feature_3231  Feature_3232  Feature_3233  Feature_3234  \\\n",
       "0    382.968383        2214.0           1.0    136.625113      0.061710   \n",
       "1    452.986164        2548.5           1.0    232.564022      0.090548   \n",
       "2    419.781765        3400.0           1.0    233.593529      0.068704   \n",
       "3    439.023968        5424.0           1.0    427.429572      0.078803   \n",
       "4    485.209184        5096.0           1.0    726.731554      0.142608   \n",
       "\n",
       "   Feature_3235  Feature_3236  Feature_3237  Feature_3238  \n",
       "0           0.0     28.154838      4.174959      0.061710  \n",
       "1           0.0     27.934229      3.931950      0.090548  \n",
       "2           0.0     27.904807      4.085035      0.068704  \n",
       "3           0.0     27.870588      4.011726      0.078803  \n",
       "4           0.0     28.846909      3.571352      0.142608  \n",
       "\n",
       "[5 rows x 3238 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace infinite values with NaN for statistics calculation\n",
    "features_clean = features.replace([np.inf, -np.inf], np.nan)\n",
    "features_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "281d02b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Feature_5</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_7</th>\n",
       "      <th>Feature_8</th>\n",
       "      <th>Feature_9</th>\n",
       "      <th>Feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature_3229</th>\n",
       "      <th>Feature_3230</th>\n",
       "      <th>Feature_3231</th>\n",
       "      <th>Feature_3232</th>\n",
       "      <th>Feature_3233</th>\n",
       "      <th>Feature_3234</th>\n",
       "      <th>Feature_3235</th>\n",
       "      <th>Feature_3236</th>\n",
       "      <th>Feature_3237</th>\n",
       "      <th>Feature_3238</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18281.541667</td>\n",
       "      <td>18432.0</td>\n",
       "      <td>9409.650391</td>\n",
       "      <td>0.514708</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.045369</td>\n",
       "      <td>2.803803</td>\n",
       "      <td>0.356658</td>\n",
       "      <td>1.803803</td>\n",
       "      <td>564.936250</td>\n",
       "      <td>...</td>\n",
       "      <td>382.968383</td>\n",
       "      <td>382.968383</td>\n",
       "      <td>2214.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>136.625113</td>\n",
       "      <td>0.061710</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.154838</td>\n",
       "      <td>4.174959</td>\n",
       "      <td>0.061710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20010.083333</td>\n",
       "      <td>20100.0</td>\n",
       "      <td>8303.049072</td>\n",
       "      <td>0.417707</td>\n",
       "      <td>0.014959</td>\n",
       "      <td>0.080294</td>\n",
       "      <td>2.338398</td>\n",
       "      <td>0.429532</td>\n",
       "      <td>1.338398</td>\n",
       "      <td>31.291507</td>\n",
       "      <td>...</td>\n",
       "      <td>452.986164</td>\n",
       "      <td>452.986164</td>\n",
       "      <td>2548.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>232.564022</td>\n",
       "      <td>0.090548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.934229</td>\n",
       "      <td>3.931950</td>\n",
       "      <td>0.090548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27260.125000</td>\n",
       "      <td>27437.0</td>\n",
       "      <td>12189.649414</td>\n",
       "      <td>0.447160</td>\n",
       "      <td>0.011428</td>\n",
       "      <td>0.046402</td>\n",
       "      <td>2.782842</td>\n",
       "      <td>0.359345</td>\n",
       "      <td>1.782842</td>\n",
       "      <td>11.965643</td>\n",
       "      <td>...</td>\n",
       "      <td>419.781765</td>\n",
       "      <td>419.781765</td>\n",
       "      <td>3400.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>233.593529</td>\n",
       "      <td>0.068704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.904807</td>\n",
       "      <td>4.085035</td>\n",
       "      <td>0.068704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41938.125000</td>\n",
       "      <td>42138.0</td>\n",
       "      <td>17866.433594</td>\n",
       "      <td>0.426019</td>\n",
       "      <td>0.009908</td>\n",
       "      <td>0.034878</td>\n",
       "      <td>3.060655</td>\n",
       "      <td>0.326727</td>\n",
       "      <td>2.060655</td>\n",
       "      <td>8.966286</td>\n",
       "      <td>...</td>\n",
       "      <td>439.023968</td>\n",
       "      <td>439.023968</td>\n",
       "      <td>5424.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>427.429572</td>\n",
       "      <td>0.078803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.870588</td>\n",
       "      <td>4.011726</td>\n",
       "      <td>0.078803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41274.125000</td>\n",
       "      <td>41439.0</td>\n",
       "      <td>14315.041992</td>\n",
       "      <td>0.346828</td>\n",
       "      <td>0.013596</td>\n",
       "      <td>0.065680</td>\n",
       "      <td>2.478506</td>\n",
       "      <td>0.403469</td>\n",
       "      <td>1.478506</td>\n",
       "      <td>34.898671</td>\n",
       "      <td>...</td>\n",
       "      <td>485.209184</td>\n",
       "      <td>485.209184</td>\n",
       "      <td>5096.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>726.731554</td>\n",
       "      <td>0.142608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.846909</td>\n",
       "      <td>3.571352</td>\n",
       "      <td>0.142608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>46787.916667</td>\n",
       "      <td>47002.0</td>\n",
       "      <td>18052.070312</td>\n",
       "      <td>0.385828</td>\n",
       "      <td>0.010883</td>\n",
       "      <td>0.042086</td>\n",
       "      <td>2.874885</td>\n",
       "      <td>0.347840</td>\n",
       "      <td>1.874885</td>\n",
       "      <td>23.499143</td>\n",
       "      <td>...</td>\n",
       "      <td>466.276055</td>\n",
       "      <td>466.276055</td>\n",
       "      <td>6064.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>585.547823</td>\n",
       "      <td>0.096561</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.787507</td>\n",
       "      <td>3.894684</td>\n",
       "      <td>0.096561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>8420.354167</td>\n",
       "      <td>8493.0</td>\n",
       "      <td>4292.039795</td>\n",
       "      <td>0.510004</td>\n",
       "      <td>0.016911</td>\n",
       "      <td>0.101797</td>\n",
       "      <td>2.145061</td>\n",
       "      <td>0.466555</td>\n",
       "      <td>1.145061</td>\n",
       "      <td>25.293867</td>\n",
       "      <td>...</td>\n",
       "      <td>383.044821</td>\n",
       "      <td>383.044821</td>\n",
       "      <td>987.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>69.155790</td>\n",
       "      <td>0.070369</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.625473</td>\n",
       "      <td>4.098452</td>\n",
       "      <td>0.070369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>37262.750000</td>\n",
       "      <td>37407.0</td>\n",
       "      <td>13950.793945</td>\n",
       "      <td>0.374390</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>0.057837</td>\n",
       "      <td>2.585819</td>\n",
       "      <td>0.386725</td>\n",
       "      <td>1.585819</td>\n",
       "      <td>37.474634</td>\n",
       "      <td>...</td>\n",
       "      <td>469.005263</td>\n",
       "      <td>469.005263</td>\n",
       "      <td>4940.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>505.566802</td>\n",
       "      <td>0.102341</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.865256</td>\n",
       "      <td>3.815115</td>\n",
       "      <td>0.102341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>25081.833333</td>\n",
       "      <td>25251.0</td>\n",
       "      <td>11689.275391</td>\n",
       "      <td>0.466045</td>\n",
       "      <td>0.011197</td>\n",
       "      <td>0.044546</td>\n",
       "      <td>2.820962</td>\n",
       "      <td>0.354489</td>\n",
       "      <td>1.820962</td>\n",
       "      <td>18.321132</td>\n",
       "      <td>...</td>\n",
       "      <td>403.597826</td>\n",
       "      <td>403.597826</td>\n",
       "      <td>2944.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>215.172554</td>\n",
       "      <td>0.073089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.180584</td>\n",
       "      <td>4.055504</td>\n",
       "      <td>0.073089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>52925.083333</td>\n",
       "      <td>53137.0</td>\n",
       "      <td>19790.341797</td>\n",
       "      <td>0.373931</td>\n",
       "      <td>0.010725</td>\n",
       "      <td>0.040871</td>\n",
       "      <td>2.903096</td>\n",
       "      <td>0.344460</td>\n",
       "      <td>1.903096</td>\n",
       "      <td>35.784277</td>\n",
       "      <td>...</td>\n",
       "      <td>479.596830</td>\n",
       "      <td>479.596830</td>\n",
       "      <td>6625.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>789.191094</td>\n",
       "      <td>0.119123</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.689509</td>\n",
       "      <td>3.765621</td>\n",
       "      <td>0.119123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>315 rows Ã— 3215 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Feature_1  Feature_2     Feature_3  Feature_4  Feature_5  Feature_6  \\\n",
       "0    18281.541667    18432.0   9409.650391   0.514708   0.011300   0.045369   \n",
       "1    20010.083333    20100.0   8303.049072   0.417707   0.014959   0.080294   \n",
       "2    27260.125000    27437.0  12189.649414   0.447160   0.011428   0.046402   \n",
       "3    41938.125000    42138.0  17866.433594   0.426019   0.009908   0.034878   \n",
       "4    41274.125000    41439.0  14315.041992   0.346828   0.013596   0.065680   \n",
       "..            ...        ...           ...        ...        ...        ...   \n",
       "310  46787.916667    47002.0  18052.070312   0.385828   0.010883   0.042086   \n",
       "311   8420.354167     8493.0   4292.039795   0.510004   0.016911   0.101797   \n",
       "312  37262.750000    37407.0  13950.793945   0.374390   0.012759   0.057837   \n",
       "313  25081.833333    25251.0  11689.275391   0.466045   0.011197   0.044546   \n",
       "314  52925.083333    53137.0  19790.341797   0.373931   0.010725   0.040871   \n",
       "\n",
       "     Feature_7  Feature_8  Feature_9  Feature_10  ...  Feature_3229  \\\n",
       "0     2.803803   0.356658   1.803803  564.936250  ...    382.968383   \n",
       "1     2.338398   0.429532   1.338398   31.291507  ...    452.986164   \n",
       "2     2.782842   0.359345   1.782842   11.965643  ...    419.781765   \n",
       "3     3.060655   0.326727   2.060655    8.966286  ...    439.023968   \n",
       "4     2.478506   0.403469   1.478506   34.898671  ...    485.209184   \n",
       "..         ...        ...        ...         ...  ...           ...   \n",
       "310   2.874885   0.347840   1.874885   23.499143  ...    466.276055   \n",
       "311   2.145061   0.466555   1.145061   25.293867  ...    383.044821   \n",
       "312   2.585819   0.386725   1.585819   37.474634  ...    469.005263   \n",
       "313   2.820962   0.354489   1.820962   18.321132  ...    403.597826   \n",
       "314   2.903096   0.344460   1.903096   35.784277  ...    479.596830   \n",
       "\n",
       "     Feature_3230  Feature_3231  Feature_3232  Feature_3233  Feature_3234  \\\n",
       "0      382.968383        2214.0           1.0    136.625113      0.061710   \n",
       "1      452.986164        2548.5           1.0    232.564022      0.090548   \n",
       "2      419.781765        3400.0           1.0    233.593529      0.068704   \n",
       "3      439.023968        5424.0           1.0    427.429572      0.078803   \n",
       "4      485.209184        5096.0           1.0    726.731554      0.142608   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "310    466.276055        6064.0           1.0    585.547823      0.096561   \n",
       "311    383.044821         987.0           1.0     69.155790      0.070369   \n",
       "312    469.005263        4940.0           1.0    505.566802      0.102341   \n",
       "313    403.597826        2944.0           1.0    215.172554      0.073089   \n",
       "314    479.596830        6625.0           1.0    789.191094      0.119123   \n",
       "\n",
       "     Feature_3235  Feature_3236  Feature_3237  Feature_3238  \n",
       "0             0.0     28.154838      4.174959      0.061710  \n",
       "1             0.0     27.934229      3.931950      0.090548  \n",
       "2             0.0     27.904807      4.085035      0.068704  \n",
       "3             0.0     27.870588      4.011726      0.078803  \n",
       "4             0.0     28.846909      3.571352      0.142608  \n",
       "..            ...           ...           ...           ...  \n",
       "310           0.0     28.787507      3.894684      0.096561  \n",
       "311           0.0     29.625473      4.098452      0.070369  \n",
       "312           0.0     26.865256      3.815115      0.102341  \n",
       "313           0.0     29.180584      4.055504      0.073089  \n",
       "314           0.0     29.689509      3.765621      0.119123  \n",
       "\n",
       "[315 rows x 3215 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_clean.dropna(axis=1, thresh=0.9 * len(features_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be0b55b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute remaining NaNs with column mean\n",
    "features_clean.fillna(features_clean.mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4166ffdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. REMOVE ZERO-VARIANCE FEATURES\n",
    "var_thresh = VarianceThreshold(threshold=0.0)\n",
    "X_var = var_thresh.fit_transform(features_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8252e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE HIGHLY CORRELATED FEATURES\n",
    "def remove_high_correlation_features(X, threshold=0.95):\n",
    "    corr_matrix = pd.DataFrame(X).corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    return pd.DataFrame(X).drop(columns=to_drop, axis=1)\n",
    "\n",
    "X_clean = remove_high_correlation_features(X_var, threshold=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc9e6b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = sm.fit_resample(X_clean, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85185a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((382, 721), (382,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_resampled.shape, y_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65a8441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 5. SCALING\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X_resampled)  # or X_clean if using correlation pruning\n",
    "\n",
    "# # 6. DIMENSIONALITY REDUCTION WITH PCA (retain 95% variance)\n",
    "# pca = PCA(n_components=20, random_state=42)\n",
    "# X_pca = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57e497b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LogisticRegression\n",
      " Fold 1: Accuracy: 0.5844, F1: 0.5897, Recall: 0.6053, Specificity: 0.5641, AUROC: 0.6059\n",
      " Fold 2: Accuracy: 0.6364, F1: 0.6585, Recall: 0.6923, Specificity: 0.5789, AUROC: 0.6653\n",
      " Fold 3: Accuracy: 0.6711, F1: 0.6377, Recall: 0.5789, Specificity: 0.7632, AUROC: 0.6884\n",
      " Fold 4: Accuracy: 0.6316, F1: 0.6410, Recall: 0.6579, Specificity: 0.6053, AUROC: 0.7043\n",
      " Fold 5: Accuracy: 0.6579, F1: 0.6486, Recall: 0.6316, Specificity: 0.6842, AUROC: 0.6988\n",
      "\n",
      "Training RandomForest\n",
      " Fold 1: Accuracy: 0.6623, F1: 0.6829, Recall: 0.7368, Specificity: 0.5897, AUROC: 0.7483\n",
      " Fold 2: Accuracy: 0.6623, F1: 0.6579, Recall: 0.6410, Specificity: 0.6842, AUROC: 0.7240\n",
      " Fold 3: Accuracy: 0.7237, F1: 0.7123, Recall: 0.6842, Specificity: 0.7632, AUROC: 0.7438\n",
      " Fold 4: Accuracy: 0.7368, F1: 0.7561, Recall: 0.8158, Specificity: 0.6579, AUROC: 0.8047\n",
      " Fold 5: Accuracy: 0.6447, F1: 0.6824, Recall: 0.7632, Specificity: 0.5263, AUROC: 0.7195\n",
      "\n",
      "Training XGBoost\n",
      " Fold 1: Accuracy: 0.6753, F1: 0.6835, Recall: 0.7105, Specificity: 0.6410, AUROC: 0.7456\n",
      " Fold 2: Accuracy: 0.6623, F1: 0.6750, Recall: 0.6923, Specificity: 0.6316, AUROC: 0.6653\n",
      " Fold 3: Accuracy: 0.6974, F1: 0.6933, Recall: 0.6842, Specificity: 0.7105, AUROC: 0.7258\n",
      " Fold 4: Accuracy: 0.6974, F1: 0.7160, Recall: 0.7632, Specificity: 0.6316, AUROC: 0.8026\n",
      " Fold 5: Accuracy: 0.6579, F1: 0.6667, Recall: 0.6842, Specificity: 0.6316, AUROC: 0.7465\n",
      "\n",
      "Training LightGBM\n",
      " Fold 1: Accuracy: 0.6623, F1: 0.6905, Recall: 0.7632, Specificity: 0.5641, AUROC: 0.7544\n",
      " Fold 2: Accuracy: 0.6883, F1: 0.6923, Recall: 0.6923, Specificity: 0.6842, AUROC: 0.7031\n",
      " Fold 3: Accuracy: 0.7237, F1: 0.7273, Recall: 0.7368, Specificity: 0.7105, AUROC: 0.7535\n",
      " Fold 4: Accuracy: 0.7500, F1: 0.7711, Recall: 0.8421, Specificity: 0.6579, AUROC: 0.8317\n",
      " Fold 5: Accuracy: 0.6842, F1: 0.7073, Recall: 0.7632, Specificity: 0.6053, AUROC: 0.7251\n",
      "\n",
      "ðŸ“Š Average Cross-Validation Results:\n",
      " LogisticRegression: Accuracy: 0.6363, F1: 0.6351, Recall: 0.6332, Specificity: 0.6391, AUROC: 0.6725\n",
      " RandomForest: Accuracy: 0.6860, F1: 0.6983, Recall: 0.7282, Specificity: 0.6443, AUROC: 0.7481\n",
      " XGBoost: Accuracy: 0.6781, F1: 0.6869, Recall: 0.7069, Specificity: 0.6493, AUROC: 0.7372\n",
      " LightGBM: Accuracy: 0.7017, F1: 0.7177, Recall: 0.7595, Specificity: 0.6444, AUROC: 0.7535\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Helper function to calculate all metrics\n",
    "def evaluate(y_true, y_pred, y_proba):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    return {\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'F1': f1_score(y_true, y_pred),\n",
    "        'Recall': recall_score(y_true, y_pred),\n",
    "        'Specificity': specificity,\n",
    "        'AUROC': roc_auc_score(y_true, y_proba)\n",
    "    }\n",
    "\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(class_weight='balanced',max_iter=1000 ,C=1,solver= 'liblinear'),\n",
    "    \"RandomForest\": RandomForestClassifier( n_estimators=100, random_state=42,max_depth=10,min_samples_leaf=1, min_samples_split=2),\n",
    "    \"XGBoost\": XGBClassifier( eval_metric='logloss', random_state=42,learning_rate=0.1, n_estimators=200, max_depth=3),\n",
    "    # \"SVM\":  SVC(kernel='linear', class_weight='balanced', probability=True, random_state=42,C= 0.001),\n",
    "    \"LightGBM\": LGBMClassifier(n_estimators= 205,max_depth= 8, learning_rate= 0.04772636524430811,num_leaves= 15,min_child_samples= 15, subsample= 0.8018697037715028,colsample_bytree= 0.6798719929743099,reg_alpha= 0.0027374388764032642,reg_lambda= 0.5886596305304316),\n",
    "}\n",
    "\n",
    "# Cross-validation setup\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = {model_name: [] for model_name in models}\n",
    "\n",
    "# Loop through models and perform cross-validation\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name}\")\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_resampled, y_resampled)):\n",
    "        X_train, X_val = X_resampled.iloc[train_idx], X_resampled.iloc[val_idx]\n",
    "        y_train, y_val = y_resampled.iloc[train_idx], y_resampled.iloc[val_idx]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        y_proba = model.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        metrics = evaluate(y_val, y_pred, y_proba)\n",
    "        results[model_name].append(metrics)\n",
    "        print(f\" Fold {fold+1}: \" + \", \".join([f\"{k}: {v:.4f}\" for k, v in metrics.items()]))\n",
    "\n",
    "# Average results\n",
    "print(\"\\nðŸ“Š Average Cross-Validation Results:\")\n",
    "for model_name, folds in results.items():\n",
    "    avg = {k: np.mean([fold[k] for fold in folds]) for k in folds[0]}\n",
    "    print(f\" {model_name}: \" + \", \".join([f\"{k}: {v:.4f}\" for k, v in avg.items()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e6258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LogisticRegression: Accuracy: 0.6363, F1: 0.6351, Recall: 0.6332, Specificity: 0.6391, AUROC: 0.6725\n",
    " RandomForest: Accuracy: 0.6860, F1: 0.6983, Recall: 0.7282, Specificity: 0.6443, AUROC: 0.7481\n",
    " XGBoost: Accuracy: 0.6781, F1: 0.6869, Recall: 0.7069, Specificity: 0.6493, AUROC: 0.7372\n",
    " LightGBM: Accuracy: 0.6808, F1: 0.6882, Recall: 0.7072, Specificity: 0.6548, AUROC: 0.7103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84302c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression: Accuracy: 0.5888, F1: 0.6191, Recall: 0.6745, Specificity: 0.5026, AUROC: 0.5946\n",
    "#  RandomForest: Accuracy: 0.6913, F1: 0.6926, Recall: 0.6966, Specificity: 0.6862, AUROC: 0.7464\n",
    "#  XGBoost: Accuracy: 0.7043, F1: 0.7162, Recall: 0.7493, Specificity: 0.6596, AUROC: 0.7442\n",
    "#  SVM: Accuracy: 0.6544, F1: 0.6601, Recall: 0.6750, Specificity: 0.6339, AUROC: 0.6654\n",
    "#  LightGBM: Accuracy: 0.6602, F1: 0.6701, Recall: 0.6916, Specificity: 0.6285, AUROC: 0.7011\n",
    "#  NaiveBayes: Accuracy: 0.5889, F1: 0.6667, Recall: 0.8270, Specificity: 0.3502, AUROC: 0.6496"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeace609",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  LogisticRegression: Accuracy: 0.5968, F1: 0.5639, Recall: 0.6603, Specificity: 0.5553, AUROC: 0.6514\n",
    "#  RandomForest: Accuracy: 0.5746, F1: 0.3480, Recall: 0.3060, Specificity: 0.7489, AUROC: 0.5858\n",
    "#  XGBoost: Accuracy: 0.5968, F1: 0.4617, Recall: 0.4517, Specificity: 0.6911, AUROC: 0.6042\n",
    "#  SVM: Accuracy: 0.6349, F1: 0.5840, Recall: 0.6447, Specificity: 0.6286, AUROC: 0.6501\n",
    "#  LightGBM: Accuracy: 0.5778, F1: 0.4734, Recall: 0.4843, Specificity: 0.6387, AUROC: 0.5941\n",
    "#  NaiveBayes: Accuracy: 0.5778, F1: 0.4417, Recall: 0.4510, Specificity: 0.6596, AUROC: 0.5943"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c360869d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 46 candidates, totalling 230 fits\n",
      "Best parameters: {'C': 1, 'class_weight': 'balanced', 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Best F1 score: 0.7091607405513077\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Build param grid\n",
    "param_grid = [\n",
    "    {\n",
    "        'penalty': ['l1'],\n",
    "        'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10],\n",
    "        'solver': ['liblinear'],\n",
    "        'class_weight': ['balanced']\n",
    "    },\n",
    "    {\n",
    "        'penalty': ['l2'],\n",
    "        'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10],\n",
    "        'solver': ['liblinear', 'lbfgs'],\n",
    "        'class_weight': ['balanced']\n",
    "    },\n",
    "    {\n",
    "        'penalty': ['elasticnet'],\n",
    "        'C': [0.01, 0.1, 1, 5],\n",
    "        'solver': ['saga'],\n",
    "        'l1_ratio': [0.1, 0.5, 0.7, 0.9],\n",
    "        'class_weight': ['balanced']\n",
    "    }\n",
    "]\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Grid search with cross-validation\n",
    "log_cv = GridSearchCV(\n",
    "    logreg,\n",
    "    param_grid,\n",
    "    scoring='f1',       # Or 'accuracy' if that's your main target\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit\n",
    "log_cv.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Output best model and params\n",
    "print(\"Best parameters:\", log_cv.best_params_)\n",
    "print(\"Best F1 score:\", log_cv.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70d15b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest params: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "rf_cv = GridSearchCV(rf, rf_grid, scoring='f1', cv=5)\n",
    "rf_cv.fit(X_resampled, y_resampled)\n",
    "\n",
    "print(\"Best Random Forest params:\", rf_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fae2bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:40:59] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:41:04] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:41:09] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:41:14] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:41:21] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:41:26] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:41:37] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:41:48] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:41:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:42:06] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:42:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:42:27] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:42:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:42:49] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:42:58] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:43:17] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:43:38] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:43:58] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:44:17] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:44:35] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:45:00] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:45:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:45:35] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:45:53] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:46:10] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:46:28] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:46:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:47:21] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:47:44] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:48:09] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:48:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:48:37] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:48:42] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:48:46] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:48:50] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:48:55] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:49:03] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:49:11] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:49:19] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:49:27] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:49:35] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:49:42] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:49:50] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:49:57] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:50:04] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:50:12] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:50:22] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:50:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:50:44] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:50:55] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:51:06] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:51:14] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:51:23] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:51:31] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:51:40] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:51:48] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:52:01] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:52:15] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:52:28] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:52:40] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:52:53] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:52:57] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:53:01] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:53:06] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:53:10] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:53:15] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:53:22] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:53:29] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:53:36] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:53:43] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:53:50] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:53:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:54:02] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:54:08] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:54:14] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:54:20] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:54:28] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:54:37] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:54:47] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:54:55] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:55:04] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:55:10] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:55:17] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:55:23] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:55:30] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:55:37] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:55:46] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:55:55] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:56:04] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:56:14] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:56:23] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "xgb = XGBClassifier(scale_pos_weight=191/124, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "xgb_cv = GridSearchCV(xgb, xgb_grid, scoring='f1', cv=5)\n",
    "xgb_cv.fit(X_resampled, y_resampled)\n",
    "\n",
    "print(\"Best XGBoost params:\", xgb_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18b05d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5832 candidates, totalling 29160 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m lgbm = LGBMClassifier(random_state=\u001b[32m42\u001b[39m)\n\u001b[32m     18\u001b[39m lgbm_cv = GridSearchCV(lgbm, lgbm_params, scoring=\u001b[33m'\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m'\u001b[39m, cv=\u001b[32m5\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m, verbose=\u001b[32m2\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mlgbm_cv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_resampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_resampled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest LGBM params:\u001b[39m\u001b[33m\"\u001b[39m, lgbm_cv.best_params_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1018\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1019\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1020\u001b[39m     )\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1027\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1028\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1571\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1569\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1570\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1571\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    963\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    964\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    965\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    966\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    967\u001b[39m         )\n\u001b[32m    968\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m970\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m    989\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    990\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    993\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgbm_params = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "lgbm = LGBMClassifier(random_state=42)\n",
    "lgbm_cv = GridSearchCV(lgbm, lgbm_params, scoring='f1', cv=5, n_jobs=-1, verbose=2)\n",
    "lgbm_cv.fit(X_resampled, y_resampled)\n",
    "print(\"Best LGBM params:\", lgbm_cv.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fcd23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\NAAMII\\Machine_learning\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-05-29 23:23:36,186] A new study created in memory with name: no-name-83c85882-6286-4eb0-bc8f-db4e470aa4a9\n",
      "[I 2025-05-29 23:23:42,435] Trial 0 finished with value: 0.6803765701372311 and parameters: {'n_estimators': 224, 'max_depth': 4, 'learning_rate': 0.07991495077906864, 'num_leaves': 30, 'min_child_samples': 30, 'subsample': 0.9723642585732775, 'colsample_bytree': 0.6676855322506077, 'reg_alpha': 0.046353388239426696, 'reg_lambda': 0.8106409126188768}. Best is trial 0 with value: 0.6803765701372311.\n",
      "[I 2025-05-29 23:23:50,343] Trial 1 finished with value: 0.6698974617673805 and parameters: {'n_estimators': 187, 'max_depth': 4, 'learning_rate': 0.024407850951624524, 'num_leaves': 53, 'min_child_samples': 12, 'subsample': 0.923126384359751, 'colsample_bytree': 0.8444733106017211, 'reg_alpha': 0.9303981408753887, 'reg_lambda': 0.47803504941692565}. Best is trial 0 with value: 0.6803765701372311.\n",
      "[I 2025-05-29 23:23:56,495] Trial 2 finished with value: 0.6782828800830186 and parameters: {'n_estimators': 249, 'max_depth': 6, 'learning_rate': 0.08130613634611003, 'num_leaves': 62, 'min_child_samples': 17, 'subsample': 0.7530193420767557, 'colsample_bytree': 0.8182273222804823, 'reg_alpha': 0.877369724783615, 'reg_lambda': 0.7322351723368041}. Best is trial 0 with value: 0.6803765701372311.\n",
      "[I 2025-05-29 23:23:59,780] Trial 3 finished with value: 0.6769380685886214 and parameters: {'n_estimators': 110, 'max_depth': 3, 'learning_rate': 0.08199281577496625, 'num_leaves': 28, 'min_child_samples': 13, 'subsample': 0.9169919036312504, 'colsample_bytree': 0.8384254509217287, 'reg_alpha': 0.2566608353142401, 'reg_lambda': 0.720770509310423}. Best is trial 0 with value: 0.6803765701372311.\n",
      "[I 2025-05-29 23:24:12,037] Trial 4 finished with value: 0.7024783182239187 and parameters: {'n_estimators': 296, 'max_depth': 5, 'learning_rate': 0.08929121636835684, 'num_leaves': 22, 'min_child_samples': 11, 'subsample': 0.9239578005155098, 'colsample_bytree': 0.8640237494886496, 'reg_alpha': 0.04078540620121818, 'reg_lambda': 0.29958810380189416}. Best is trial 4 with value: 0.7024783182239187.\n",
      "[I 2025-05-29 23:24:19,136] Trial 5 finished with value: 0.6996154069781146 and parameters: {'n_estimators': 152, 'max_depth': 4, 'learning_rate': 0.0841735690239807, 'num_leaves': 63, 'min_child_samples': 24, 'subsample': 0.9234265206393868, 'colsample_bytree': 0.849715744640548, 'reg_alpha': 0.5264591105580452, 'reg_lambda': 0.7719049874855737}. Best is trial 4 with value: 0.7024783182239187.\n",
      "[I 2025-05-29 23:24:25,751] Trial 6 finished with value: 0.6826797638898674 and parameters: {'n_estimators': 172, 'max_depth': 9, 'learning_rate': 0.06099246631859057, 'num_leaves': 47, 'min_child_samples': 30, 'subsample': 0.9945388396618673, 'colsample_bytree': 0.8324004647567308, 'reg_alpha': 0.524973394208418, 'reg_lambda': 0.6681806658581794}. Best is trial 4 with value: 0.7024783182239187.\n",
      "[I 2025-05-29 23:24:34,462] Trial 7 finished with value: 0.6755119783938783 and parameters: {'n_estimators': 295, 'max_depth': 9, 'learning_rate': 0.03092234670354832, 'num_leaves': 24, 'min_child_samples': 25, 'subsample': 0.7219801638288846, 'colsample_bytree': 0.8912113355984942, 'reg_alpha': 0.32767527037707234, 'reg_lambda': 0.5319009914445665}. Best is trial 4 with value: 0.7024783182239187.\n",
      "[I 2025-05-29 23:24:39,352] Trial 8 finished with value: 0.6874032642582065 and parameters: {'n_estimators': 217, 'max_depth': 6, 'learning_rate': 0.08504138405472088, 'num_leaves': 55, 'min_child_samples': 16, 'subsample': 0.7123289149441916, 'colsample_bytree': 0.7956872996677042, 'reg_alpha': 0.9980945720090003, 'reg_lambda': 0.5242097331370374}. Best is trial 4 with value: 0.7024783182239187.\n",
      "[I 2025-05-29 23:24:44,420] Trial 9 finished with value: 0.6964905145427234 and parameters: {'n_estimators': 190, 'max_depth': 7, 'learning_rate': 0.05250320496243469, 'num_leaves': 58, 'min_child_samples': 27, 'subsample': 0.9945138087843951, 'colsample_bytree': 0.6758639014497521, 'reg_alpha': 0.8009511360216562, 'reg_lambda': 0.25067875637255765}. Best is trial 4 with value: 0.7024783182239187.\n",
      "[I 2025-05-29 23:24:52,891] Trial 10 finished with value: 0.6783006018371871 and parameters: {'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.09935469854573434, 'num_leaves': 17, 'min_child_samples': 10, 'subsample': 0.8235556433988798, 'colsample_bytree': 0.9962455797862014, 'reg_alpha': 0.08611583309131118, 'reg_lambda': 0.09184471788422854}. Best is trial 4 with value: 0.7024783182239187.\n",
      "[I 2025-05-29 23:24:58,222] Trial 11 finished with value: 0.6807981259247082 and parameters: {'n_estimators': 143, 'max_depth': 5, 'learning_rate': 0.09792462389464048, 'num_leaves': 40, 'min_child_samples': 22, 'subsample': 0.6237558343097938, 'colsample_bytree': 0.9344610334194378, 'reg_alpha': 0.6041727617826635, 'reg_lambda': 0.9880722081972247}. Best is trial 4 with value: 0.7024783182239187.\n",
      "[I 2025-05-29 23:25:04,212] Trial 12 finished with value: 0.7033984639845572 and parameters: {'n_estimators': 145, 'max_depth': 3, 'learning_rate': 0.06436014399171364, 'num_leaves': 37, 'min_child_samples': 21, 'subsample': 0.8578463238677939, 'colsample_bytree': 0.753379362542153, 'reg_alpha': 0.675458423725608, 'reg_lambda': 0.3395682625829273}. Best is trial 12 with value: 0.7033984639845572.\n",
      "[I 2025-05-29 23:25:09,721] Trial 13 finished with value: 0.683278807717379 and parameters: {'n_estimators': 259, 'max_depth': 3, 'learning_rate': 0.05889910642015302, 'num_leaves': 37, 'min_child_samples': 19, 'subsample': 0.8455091063007141, 'colsample_bytree': 0.7424610684603707, 'reg_alpha': 0.6906833902740711, 'reg_lambda': 0.27194313974670736}. Best is trial 12 with value: 0.7033984639845572.\n",
      "[I 2025-05-29 23:25:15,398] Trial 14 finished with value: 0.6700565423916525 and parameters: {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.04159026343912535, 'num_leaves': 18, 'min_child_samples': 20, 'subsample': 0.8720018333581078, 'colsample_bytree': 0.7491275227113076, 'reg_alpha': 0.3130836966911882, 'reg_lambda': 0.30687962530788815}. Best is trial 12 with value: 0.7033984639845572.\n",
      "[I 2025-05-29 23:25:18,834] Trial 15 finished with value: 0.6697942390027475 and parameters: {'n_estimators': 140, 'max_depth': 3, 'learning_rate': 0.06875403224071254, 'num_leaves': 38, 'min_child_samples': 15, 'subsample': 0.7725618423771674, 'colsample_bytree': 0.6242635363683003, 'reg_alpha': 0.7156919544702541, 'reg_lambda': 0.05332273608804017}. Best is trial 12 with value: 0.7033984639845572.\n",
      "[I 2025-05-29 23:25:26,338] Trial 16 finished with value: 0.6869697221847619 and parameters: {'n_estimators': 263, 'max_depth': 5, 'learning_rate': 0.07074203678618604, 'num_leaves': 45, 'min_child_samples': 18, 'subsample': 0.8777212016423253, 'colsample_bytree': 0.7582750069512059, 'reg_alpha': 0.18247917852927428, 'reg_lambda': 0.37028969187006733}. Best is trial 12 with value: 0.7033984639845572.\n",
      "[I 2025-05-29 23:25:31,476] Trial 17 finished with value: 0.690305097761238 and parameters: {'n_estimators': 127, 'max_depth': 8, 'learning_rate': 0.04488193960716086, 'num_leaves': 32, 'min_child_samples': 22, 'subsample': 0.8015105077002773, 'colsample_bytree': 0.9051518834834784, 'reg_alpha': 0.4167965656493047, 'reg_lambda': 0.1862560732896062}. Best is trial 12 with value: 0.7033984639845572.\n",
      "[I 2025-05-29 23:25:36,392] Trial 18 finished with value: 0.6791900637670125 and parameters: {'n_estimators': 163, 'max_depth': 4, 'learning_rate': 0.06863072924079769, 'num_leaves': 23, 'min_child_samples': 14, 'subsample': 0.877559058632201, 'colsample_bytree': 0.7186333858592455, 'reg_alpha': 0.653249113530193, 'reg_lambda': 0.40908477895292905}. Best is trial 12 with value: 0.7033984639845572.\n",
      "[I 2025-05-29 23:25:46,105] Trial 19 finished with value: 0.6801148866189516 and parameters: {'n_estimators': 216, 'max_depth': 5, 'learning_rate': 0.015029234070241745, 'num_leaves': 22, 'min_child_samples': 10, 'subsample': 0.9320269207076575, 'colsample_bytree': 0.9891367463417842, 'reg_alpha': 0.45142652495296753, 'reg_lambda': 0.6052203485200707}. Best is trial 12 with value: 0.7033984639845572.\n",
      "[I 2025-05-29 23:25:51,386] Trial 20 finished with value: 0.6873803255655455 and parameters: {'n_estimators': 277, 'max_depth': 3, 'learning_rate': 0.09334015710720943, 'num_leaves': 34, 'min_child_samples': 21, 'subsample': 0.6563296264480861, 'colsample_bytree': 0.7801922563811035, 'reg_alpha': 0.0033549570775429707, 'reg_lambda': 0.179100403214604}. Best is trial 12 with value: 0.7033984639845572.\n",
      "[I 2025-05-29 23:25:55,967] Trial 21 finished with value: 0.6901450327153138 and parameters: {'n_estimators': 161, 'max_depth': 4, 'learning_rate': 0.08748677800732038, 'num_leaves': 46, 'min_child_samples': 25, 'subsample': 0.9389478856462988, 'colsample_bytree': 0.8758998517495343, 'reg_alpha': 0.5442038381353326, 'reg_lambda': 0.9488482505168399}. Best is trial 12 with value: 0.7033984639845572.\n",
      "[I 2025-05-29 23:26:00,356] Trial 22 finished with value: 0.6732062601180319 and parameters: {'n_estimators': 127, 'max_depth': 4, 'learning_rate': 0.07407009074156155, 'num_leaves': 51, 'min_child_samples': 25, 'subsample': 0.8430203995366355, 'colsample_bytree': 0.9466485076562158, 'reg_alpha': 0.7839265868298538, 'reg_lambda': 0.8524554821548227}. Best is trial 12 with value: 0.7033984639845572.\n",
      "[I 2025-05-29 23:26:05,333] Trial 23 finished with value: 0.6931596652806903 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.089690288798291, 'num_leaves': 63, 'min_child_samples': 23, 'subsample': 0.8925621628964624, 'colsample_bytree': 0.8627628415573063, 'reg_alpha': 0.4099038670898361, 'reg_lambda': 0.4123695945655569}. Best is trial 12 with value: 0.7033984639845572.\n",
      "[I 2025-05-29 23:26:10,479] Trial 24 finished with value: 0.7018629044051933 and parameters: {'n_estimators': 175, 'max_depth': 6, 'learning_rate': 0.07738813187437785, 'num_leaves': 43, 'min_child_samples': 28, 'subsample': 0.9524911545342627, 'colsample_bytree': 0.7982651108015594, 'reg_alpha': 0.16022817853264215, 'reg_lambda': 0.3418204883385413}. Best is trial 12 with value: 0.7033984639845572.\n",
      "[I 2025-05-29 23:26:15,321] Trial 25 finished with value: 0.6852234432234432 and parameters: {'n_estimators': 185, 'max_depth': 6, 'learning_rate': 0.07535441225297859, 'num_leaves': 43, 'min_child_samples': 28, 'subsample': 0.9610130750053442, 'colsample_bytree': 0.7046917615066631, 'reg_alpha': 0.14633684185835316, 'reg_lambda': 0.3325608410034175}. Best is trial 12 with value: 0.7033984639845572.\n",
      "[I 2025-05-29 23:26:21,829] Trial 26 finished with value: 0.6873388180996496 and parameters: {'n_estimators': 239, 'max_depth': 7, 'learning_rate': 0.06418893192104329, 'num_leaves': 27, 'min_child_samples': 27, 'subsample': 0.8999756366054965, 'colsample_bytree': 0.7940602936561364, 'reg_alpha': 0.16305436763498243, 'reg_lambda': 0.17523106022456925}. Best is trial 12 with value: 0.7033984639845572.\n",
      "[I 2025-05-29 23:26:29,153] Trial 27 finished with value: 0.7026650692827595 and parameters: {'n_estimators': 175, 'max_depth': 6, 'learning_rate': 0.05274834685437475, 'num_leaves': 15, 'min_child_samples': 12, 'subsample': 0.8455983956638558, 'colsample_bytree': 0.7738308356871993, 'reg_alpha': 0.24046928964157743, 'reg_lambda': 0.45710868094769685}. Best is trial 12 with value: 0.7033984639845572.\n",
      "[I 2025-05-29 23:26:37,099] Trial 28 finished with value: 0.6935897435897436 and parameters: {'n_estimators': 201, 'max_depth': 8, 'learning_rate': 0.05188788436358379, 'num_leaves': 16, 'min_child_samples': 11, 'subsample': 0.846514710336509, 'colsample_bytree': 0.7131146209487534, 'reg_alpha': 0.25331470366241626, 'reg_lambda': 0.4516044991007125}. Best is trial 12 with value: 0.7033984639845572.\n",
      "[I 2025-05-29 23:26:44,362] Trial 29 finished with value: 0.6989911853636037 and parameters: {'n_estimators': 228, 'max_depth': 5, 'learning_rate': 0.04190912556210383, 'num_leaves': 21, 'min_child_samples': 13, 'subsample': 0.8591812472312416, 'colsample_bytree': 0.6072157902722563, 'reg_alpha': 0.069113255278599, 'reg_lambda': 0.002446009464500465}. Best is trial 12 with value: 0.7033984639845572.\n",
      "[I 2025-05-29 23:26:52,186] Trial 30 finished with value: 0.7210126734634894 and parameters: {'n_estimators': 205, 'max_depth': 8, 'learning_rate': 0.04772636524430811, 'num_leaves': 15, 'min_child_samples': 15, 'subsample': 0.8018697037715028, 'colsample_bytree': 0.6798719929743099, 'reg_alpha': 0.0027374388764032642, 'reg_lambda': 0.5886596305304316}. Best is trial 30 with value: 0.7210126734634894.\n",
      "[I 2025-05-29 23:27:00,656] Trial 31 finished with value: 0.7011432743962864 and parameters: {'n_estimators': 206, 'max_depth': 7, 'learning_rate': 0.047960058505995835, 'num_leaves': 15, 'min_child_samples': 12, 'subsample': 0.8108131234024378, 'colsample_bytree': 0.6558269756112107, 'reg_alpha': 0.016937993339772972, 'reg_lambda': 0.5777481042811459}. Best is trial 30 with value: 0.7210126734634894.\n",
      "[I 2025-05-29 23:27:11,209] Trial 32 finished with value: 0.7070417485087976 and parameters: {'n_estimators': 282, 'max_depth': 8, 'learning_rate': 0.03528863653432562, 'num_leaves': 20, 'min_child_samples': 15, 'subsample': 0.7803566998977968, 'colsample_bytree': 0.674510830438346, 'reg_alpha': 0.09764819513758247, 'reg_lambda': 0.6254035071188588}. Best is trial 30 with value: 0.7210126734634894.\n",
      "[I 2025-05-29 23:27:16,503] Trial 33 finished with value: 0.698494397169096 and parameters: {'n_estimators': 123, 'max_depth': 8, 'learning_rate': 0.03426877421211103, 'num_leaves': 20, 'min_child_samples': 16, 'subsample': 0.7694085954091777, 'colsample_bytree': 0.6688123201412995, 'reg_alpha': 0.10683714596886892, 'reg_lambda': 0.6260535887391196}. Best is trial 30 with value: 0.7210126734634894.\n",
      "[I 2025-05-29 23:27:23,383] Trial 34 finished with value: 0.7097927716580159 and parameters: {'n_estimators': 183, 'max_depth': 9, 'learning_rate': 0.03327030897593311, 'num_leaves': 27, 'min_child_samples': 18, 'subsample': 0.7781957586958892, 'colsample_bytree': 0.6913448658195115, 'reg_alpha': 0.23889849592944623, 'reg_lambda': 0.48689685425057944}. Best is trial 30 with value: 0.7210126734634894.\n",
      "[I 2025-05-29 23:27:31,045] Trial 35 finished with value: 0.7082631437676871 and parameters: {'n_estimators': 236, 'max_depth': 9, 'learning_rate': 0.024261954846689596, 'num_leaves': 26, 'min_child_samples': 18, 'subsample': 0.7290891468970008, 'colsample_bytree': 0.6398675615935439, 'reg_alpha': 0.33805618137742605, 'reg_lambda': 0.5316320625731079}. Best is trial 30 with value: 0.7210126734634894.\n",
      "[I 2025-05-29 23:27:38,712] Trial 36 finished with value: 0.6921129503407984 and parameters: {'n_estimators': 245, 'max_depth': 9, 'learning_rate': 0.016584394190636245, 'num_leaves': 26, 'min_child_samples': 18, 'subsample': 0.7270949179948022, 'colsample_bytree': 0.6295575691890473, 'reg_alpha': 0.3399508445135098, 'reg_lambda': 0.6902300971679022}. Best is trial 30 with value: 0.7210126734634894.\n",
      "[I 2025-05-29 23:27:47,272] Trial 37 finished with value: 0.6943073593073593 and parameters: {'n_estimators': 230, 'max_depth': 10, 'learning_rate': 0.02459889919243846, 'num_leaves': 30, 'min_child_samples': 17, 'subsample': 0.7487620351571994, 'colsample_bytree': 0.6813891207498517, 'reg_alpha': 0.23384832808984715, 'reg_lambda': 0.5549901765942016}. Best is trial 30 with value: 0.7210126734634894.\n",
      "[I 2025-05-29 23:27:56,781] Trial 38 finished with value: 0.6959119659097451 and parameters: {'n_estimators': 274, 'max_depth': 9, 'learning_rate': 0.03442928657840466, 'num_leaves': 25, 'min_child_samples': 15, 'subsample': 0.6811665587770834, 'colsample_bytree': 0.6956951058326067, 'reg_alpha': 0.10337471855633334, 'reg_lambda': 0.8083820475098508}. Best is trial 30 with value: 0.7210126734634894.\n",
      "[I 2025-05-29 23:28:05,064] Trial 39 finished with value: 0.6716039235026577 and parameters: {'n_estimators': 283, 'max_depth': 8, 'learning_rate': 0.022387034813426722, 'num_leaves': 19, 'min_child_samples': 19, 'subsample': 0.7794434335916617, 'colsample_bytree': 0.6417933836901235, 'reg_alpha': 0.29413895059542683, 'reg_lambda': 0.6610647512930801}. Best is trial 30 with value: 0.7210126734634894.\n",
      "[I 2025-05-29 23:28:12,384] Trial 40 finished with value: 0.7115316285641489 and parameters: {'n_estimators': 211, 'max_depth': 10, 'learning_rate': 0.027182550808956294, 'num_leaves': 29, 'min_child_samples': 17, 'subsample': 0.7377041360128576, 'colsample_bytree': 0.650168402321614, 'reg_alpha': 0.19796043667012592, 'reg_lambda': 0.7442062570623362}. Best is trial 30 with value: 0.7210126734634894.\n",
      "[I 2025-05-29 23:28:19,441] Trial 41 finished with value: 0.7000695402790539 and parameters: {'n_estimators': 192, 'max_depth': 10, 'learning_rate': 0.028153475386201998, 'num_leaves': 29, 'min_child_samples': 17, 'subsample': 0.7416683489637429, 'colsample_bytree': 0.6525702341714086, 'reg_alpha': 0.19872007651813622, 'reg_lambda': 0.7362564450213078}. Best is trial 30 with value: 0.7210126734634894.\n",
      "[I 2025-05-29 23:28:27,616] Trial 42 finished with value: 0.7079182400392652 and parameters: {'n_estimators': 209, 'max_depth': 9, 'learning_rate': 0.03623858716292706, 'num_leaves': 33, 'min_child_samples': 14, 'subsample': 0.7054995709538021, 'colsample_bytree': 0.6184067795245362, 'reg_alpha': 0.1213767341735138, 'reg_lambda': 0.4839152153597615}. Best is trial 30 with value: 0.7210126734634894.\n",
      "[I 2025-05-29 23:28:35,207] Trial 43 finished with value: 0.7104954721543661 and parameters: {'n_estimators': 211, 'max_depth': 9, 'learning_rate': 0.020290665284094254, 'num_leaves': 33, 'min_child_samples': 14, 'subsample': 0.6966982993562605, 'colsample_bytree': 0.6046640055607466, 'reg_alpha': 0.3694025421438371, 'reg_lambda': 0.49214425561559433}. Best is trial 30 with value: 0.7210126734634894.\n",
      "[I 2025-05-29 23:28:42,890] Trial 44 finished with value: 0.6867323312648919 and parameters: {'n_estimators': 218, 'max_depth': 9, 'learning_rate': 0.020237545856689323, 'num_leaves': 35, 'min_child_samples': 16, 'subsample': 0.6903028598816788, 'colsample_bytree': 0.6409536680186313, 'reg_alpha': 0.385995931444071, 'reg_lambda': 0.5218684708491986}. Best is trial 30 with value: 0.7210126734634894.\n",
      "[I 2025-05-29 23:28:49,063] Trial 45 finished with value: 0.6940424990458244 and parameters: {'n_estimators': 196, 'max_depth': 10, 'learning_rate': 0.027605679870949668, 'num_leaves': 31, 'min_child_samples': 18, 'subsample': 0.6617302893018652, 'colsample_bytree': 0.6098112522752056, 'reg_alpha': 0.4812268806387886, 'reg_lambda': 0.5696241414244814}. Best is trial 30 with value: 0.7210126734634894.\n",
      "[I 2025-05-29 23:28:56,674] Trial 46 finished with value: 0.679909406800695 and parameters: {'n_estimators': 235, 'max_depth': 9, 'learning_rate': 0.012013694301671277, 'num_leaves': 27, 'min_child_samples': 19, 'subsample': 0.7381917643618688, 'colsample_bytree': 0.7241093492115693, 'reg_alpha': 0.37317057104214835, 'reg_lambda': 0.7606793022550883}. Best is trial 30 with value: 0.7210126734634894.\n",
      "[I 2025-05-29 23:29:04,955] Trial 47 finished with value: 0.7014533027947663 and parameters: {'n_estimators': 220, 'max_depth': 10, 'learning_rate': 0.019245236326257877, 'num_leaves': 24, 'min_child_samples': 14, 'subsample': 0.7630735449435679, 'colsample_bytree': 0.6918031676039283, 'reg_alpha': 0.34286728031093155, 'reg_lambda': 0.8866390120163443}. Best is trial 30 with value: 0.7210126734634894.\n",
      "[I 2025-05-29 23:29:12,619] Trial 48 finished with value: 0.696835930953578 and parameters: {'n_estimators': 253, 'max_depth': 9, 'learning_rate': 0.03098254868163662, 'num_leaves': 35, 'min_child_samples': 20, 'subsample': 0.6296258209056134, 'colsample_bytree': 0.6591441722004657, 'reg_alpha': 0.2816475651362501, 'reg_lambda': 0.4983065747623512}. Best is trial 30 with value: 0.7210126734634894.\n",
      "[I 2025-05-29 23:29:19,982] Trial 49 finished with value: 0.6892887722548112 and parameters: {'n_estimators': 185, 'max_depth': 10, 'learning_rate': 0.039924216154448466, 'num_leaves': 28, 'min_child_samples': 17, 'subsample': 0.7899640437163322, 'colsample_bytree': 0.6000018865424722, 'reg_alpha': 0.21706908832694854, 'reg_lambda': 0.6933558711951789}. Best is trial 30 with value: 0.7210126734634894.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 Score: 0.7210126734634894\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Study' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m study.optimize(objective, n_trials=\u001b[32m50\u001b[39m)  \u001b[38;5;66;03m# You can increase n_trials for more accuracy\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest F1 Score:\u001b[39m\u001b[33m\"\u001b[39m, study.best_value)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest Hyperparameters:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbest_params_\u001b[49m)\n\u001b[32m     35\u001b[39m best_model = LGBMClassifier(**study.best_params_, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m     36\u001b[39m best_model.fit(X_resampled, y_resampled)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Study' object has no attribute 'best_params_'"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 15, 64),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 30),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "        'class_weight': 'balanced',\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'verbosity': -1\n",
    "    }\n",
    "\n",
    "    model = LGBMClassifier(**params)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    f1 = cross_val_score(model, X_resampled, y_resampled, cv=skf, scoring=make_scorer(f1_score)).mean()\n",
    "    return f1\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)  # You can increase n_trials for more accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b9cab1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optuna' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43moptuna\u001b[49m.visualization.plot_optimization_history(study).show()\n\u001b[32m      2\u001b[39m optuna.visualization.plot_param_importances(study).show()\n",
      "\u001b[31mNameError\u001b[39m: name 'optuna' is not defined"
     ]
    }
   ],
   "source": [
    "optuna.visualization.plot_optimization_history(study).show()\n",
    "optuna.visualization.plot_param_importances(study).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11cb56ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 Score: 0.7210126734634894\n",
      "Best Hyperparameters: {'n_estimators': 205, 'max_depth': 8, 'learning_rate': 0.04772636524430811, 'num_leaves': 15, 'min_child_samples': 15, 'subsample': 0.8018697037715028, 'colsample_bytree': 0.6798719929743099, 'reg_alpha': 0.0027374388764032642, 'reg_lambda': 0.5886596305304316}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Study' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest F1 Score:\u001b[39m\u001b[33m\"\u001b[39m, study.best_value)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest Hyperparameters:\u001b[39m\u001b[33m\"\u001b[39m, study.best_params)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m best_model = LGBMClassifier(**\u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbest_params_\u001b[49m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m      5\u001b[39m best_model.fit(X_resampled, y_resampled)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Study' object has no attribute 'best_params_'"
     ]
    }
   ],
   "source": [
    "print(\"Best F1 Score:\", study.best_value)\n",
    "print(\"Best Hyperparameters:\", study.best_params)\n",
    "\n",
    "best_model = LGBMClassifier(**study.best_params_, random_state=42)\n",
    "best_model.fit(X_resampled, y_resampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78f71d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
